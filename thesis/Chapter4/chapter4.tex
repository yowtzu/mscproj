\chapter{Portfolio optimisation}
\graphicspath{{Chapter4/figures/}}
\label{StaticSecurityPolicyInference}
\label{ch:experiment}

\section{Technical Approach}
Traditionally, multi-period mean-variance portfolio optimization have been explired in an analytical fashion, adopting necessary assumption as necessary. This seems rather restrictive; there are many instances where numerical method has been used to derive an approximate or even more effective solution to the problem in question. For example, Monte Carlo technique is used to do integral, evolutionary techniques applied in engineering domains, etc..

Our approach to the problem in this thesis is a radical one. We view a portfolio optimisation as a \emph{stochastic} control problem. We adopt the Bayesian view and treat tehse parameters as random variables. The objective is to find the sequence of control parameters that minimize the control objective defined in terms of portfolio return and financial risk. We investigate the potential of using SMCs as the means of determining the optimal strategy, or at least excellent, strategies for multi-period mean-variance portfolio optimisation problem. The main reason of choosing SMCs is its ability to carry out \emph{sequential} update on the posterior distribution over time fit well with parameter inference in stochastic process. Moreoever, these techniques have achieved significant success in their applications on many domains. Of course, other heuristic search tecniques are also potentially applicable.

%We investigate in this chapter how SMCs can be used to search for the optimal control parameters for portfolio optimisation problem in financial market, i.e., what decision a portfolio manager need to do to minimize the portfolio's tracking error to the target's stock index over a finite time horizon. This chapter is organised as follows: In Section \ref{sec:po}  shows how a portfolio optimisation problem can be re-casted into a parameter estimation problem in SMC framework. Section \ref{sec:toy} details the experiments in accessing how well this approach perform to track various target signals generated by some known models. Section \ref{sec:real} presents the experiments of examining the approach using real-world data collected from Yahoo finance. Section \ref{sec:extension} details possible extensions that can be easily achieved with this framework with some concrete examples. Finally, Section \ref{sec:summary} concludes this chapter with a summary of results.

\section{Portfolio Optimisation}
\label{sec:po}
Resource allocation is a common challenge for every investor. In the investment decision making process, investors decide how much resources are allocated to different investable assets to form a portfolio with the aim to optimise the performance of the overall portfolio is better off than any other according to some criterion. The criterion can be different to the investors. (Some investors may have different considerations such as tax considerations and legal restriction on investment assets or holding periods.)

Two common objectives, often contradicting, are the financial return and the investment risk. The Markowitz's modern portfolio theory \cite{HM52} proposes a portfolio selection framework. In Markowitz's model, it is assumed that investor attempt to maximize a portfolio's return and minimize the risk (measured by the variance of the portfolio re turn. Based on this criteria, the set of non-dominance portfolio is known as the \emph{efficient porfolios}. Using variance as a ris measure has its limitation. Variance is a symmetric measure; an out performing asset (than the expected return) is deemed to as risky as an under perfoming one. Many alternatve risk measurements have been proposed, e.g., Sortino ratio, Conditional Value at Risk (CVaR), etc.. Refer \cite{RTR00} for details.

In the original Marowitz model, the investment decision problem is viewed and solved as a single time-step problem. In practice, investment often span across multi time-step period and adjustments may be made to the allocation periodically to achive better performance as needed. This problem is much more difficult to deal with because it is time inconsistent in the sense that an investment strategy that is optimal over the whole period may not be the optimal one over a sub-interval of the period. This violates the Bellman's Priciple of Optimality \cite{BR57}. Consequently, dynamic programming approach is not applicable here.

\
For index tracker fund manager, the main objective of portfolio management is to track and replicate the exposure of a benchmark index\footnote{The lack of active management generally makes the fund less vulnerable to change of management and has the advatanges of lower fees and taxes}.
Different investors have different risk appetite and goals, yet it is safe assumed that investor attempt to maximize a portfolio's return and minimize the risk (measured by the variance of the portfolio return. However, there are other constraint an investor need to consider in practice, e.g., asset type, holiding periods, etc. 

Different metrics have been introduced to quantify the mismatch between the performance of a fund and its benchmark. For example, tracking difference is the sum of absolute difference in returns between of a fund and its benchmark. Here, we adopt the tracking error as our metric, which is defined to be the standard deviation of the absolute difference of the returns of the fund and the benchmark defined in \cite{BJ13}. Formally, tracking error is defined to be:
\begin{equation}
  \epsilon = \sqrt(E[(r_p - r_b)^2])
\end{equation}

The tracking error can be caused by differnet factors: some of which can be summarised as follows:
\begin{enumerate}
\item Benchmark index rebalance --- the benchmark index is re-weighting its constitutents periodically to reflex the changes on the market based on its methodology. To track the index, the fund has to adjust its portfolio accordingly. This will incur some trasaction costs. During the index rebalance period, cash drag can happen between the liquidation of the constituents that have weights reduced/dropped and the addition of the constituents that have weights increased/added. This cash is essentially not participating in the market and therefore do not reflex the changes on the benchmark index.
\item different assumption on dividend trinvestment and taxation --- This is best illustrate with examples. For example, the benchmark index calculation may assume immediate reinvestment of dividends on ex-dividend dates but the fund may only able to reinvest the dividend after receiving it. The tax treatment may on the dividends amy also different too.
\item Replication and Sampling techniques --- funds may choose too replicate the benchmakr index by selecting a subset of the constituents (often the ones with larger weights and more liquid) in an effort to minimize the transaction costs. This exclusion of the smaller, less liquid constitutents may introduce another source od tracking error, especially under stressed market.
\item Total Expense Ratio --- the average annual expense that is charged to the fund on daily basis to cover the management cost.
\end{enumerate}
This list is by no mean exclusive. See \cite{} for further detail.

\section{Portfolio Optimisation as a Stochastic Control Problem}
Traditionally, the state space models used in portfolio management are deterministic. For example, forcasting and predict f[29, 30], valuing electricity contracts for hedging in deregulated electricity markets, the short term available wind power and the temperature driven consumer demand (see [28, 29, 30] and the references within), or when examining the power transfer fluctuations across transmission lines [27]. TeIt seems there is a pressing interest for stochastic modelling from computational statistics when analytical solution is not available. 

We focus here the problem in minimizing the tracking error between a portfolio and its benchmark index using a stochastic control modelling approach. Our aim is to determine what investment actions (buy or sell) on the necessary constituents a portfolio manager has to do on a daily basis across the investment horizon to minimize the tracking error of the fund managed. We will proceed by presenting the stochastic state space model that we assume throughout this thesis. This model is by no mean to compete the state of the art model in realistic portfolio optimisation, but to motivate further work in this direction.

MOdel and objective function

\section{Technical Approach}
The technical approach is to 

Portfolio managers is facing constant challenge

\section{Objective in portfolio optimisation}
We adopt the conditional linear Gaussian model with the following form:
\begin{align}
%  U_t \sim P(U_t \mid U_{t-1}) \\
  X_t = A_t(U_t)X_{t-1} + B_t{U_t}W_t + F_t{U_t}, & W_t \sim N(0,I) \\
  Y_t = C_t(U_t)X_{t-1} + D_t{U_t}V_t + G_t{U_t}, & N_t \sim N(0,I)
\label{eq:gaussianmodel}
\end{align}
where $A_n$,$B_n$, $C_n$, $D_n$, $F_n$, and $G_n$ are appropriate matrix/vector functions, ${U_t}_{t \geq 0}$ is a determinstic control input sequence that is used regulate the hidden states, ${X_t}_{t \geq 0}$, which are assumed to be a discrete time hidden Markov process that has an initial value $x_0$ and admit Gaussian transition density $f_t(x_t \mid x_{t_1}, u_t$ and ${Y_t}_{t \geq 0}$ is the only observable process which has a Gaussian conditional likelihood density $g_n(y_n \mid x_n, u_n)$.

With this model, the objective is to search for a sequence of controls $u_{1:t}$ that would result in a sequence of observations $y_{1:t}$ is closed to a reference signal $y^{ref}_{1:T}$. This problem is often known as stochastic regulation problem. We adopt here the following finite horizon multiplicative reward function:
\begin{equation}
1+1
 % J(u_{1:T},y^{ref}_{1:T}, x_0)  = \E_{x_0}\left[\exp \left( -\frac{1}{2} \sum^T_{n=1} \left \|y^{ref}_{n} - C_n(u_n)X_n - G_n(u_n) \|^2_{Q_n(u_n}^{-1} + \| u_n - Pu_{n-1} \|^2_{L_n} \right) \right) \right]
\end{equation}
where the expectation is take with respect to the whole path of the Markov Chain $X_{0:T}$, i.e., $E_{x_0}[f(X_{1:t})] = \int f(X_{1:T}) \prod f_t(x_t \mid x_{t-1})~dx_{1:t}$, with $Q_n$, $L_n$ are assumed to be known. The corresponding optimal open loop policy is:
\begin{equation}
  u^*_{1:T} = \arg\max_{u_{1:T}} J(u_{1:T};y^{ref}_{1:t};x_0)
\end{equation}

\section{Technical approach}
Under the Bayesian inference framework, we treat the control inputs as random variables that admit a prior distribution. Assuming the sequenc

The key point to note is that for the model becomes a linear Gussian model for a given $u_t$, which allows us to solve $x_t$ analytically using Kalman Filter algorithm.

Under the realm of Bayesian inference framework, we treat $U$ as a random variable and admit a probability distribution. The objective is to compute the marginal posterior distribution density $p(u_{0:t} \mid y_{0:t}$.

This desntiy function can be derived as follows:
\begin{align}
  -1+1
\end{align}
We can solve this equation using SMC by consider the pair etc.

However, a more efficient algorithm can be derived by considering the following factorisation:
\begin{equation}
  p(x_{0:t}, u_{0:t} \mid y_{1:t}) = p(x_{0:t} \mid u_{0:t}, y_{1:t}) p(u_{0:t} \mid y_{1:t})
\end{equation}

Note that density $P(x_{0:t} \mid u_{0:t}, y_{1:t}$ is Gaussian mixture model, which can be computed analytically usign Kalman Filfter given the density $p(u_{0:t} \mid y_{1:t}$, which has the following recursion form:
\begin{equation}
 1+1
\end{equation}
This can be resolved using SMC by using particles to do this.


\section{Problem formulation}

Assume for the time being that the control inputs are set as U1:n = u1:n and remain fixed. Recall {Xn}n0 and {Yn}n1 are assumed to be stochastic processes obeying a Markov transition density fn(xn|xn1,un) and a conditionally independent likelihood density gn(yn|xn,un) respectively. Given any observed y1:n realisation, inference about the states X1:n may be based on the following posterior density


show the algorithm


\section{Numerical example on stationary oscillating wave}
We will consider a simple linear Gaussian state space model as presented earlier as \eqref{eq:gaussianmodel}, with $A_t=B_t=C_t=D_t=I$, $F_t{u_t}=u_t$, $G_t{u_t}=0$, $Xi = 0$, $u=0$. This model can be re-written as follows:
\begin{align}
  X_t = X_{t-1} + W_t + U_t, & W_t \sim N(0,I) \\
Y_t = X_{t-1} + V_t, & N_t \sim N(0,I)
\label{eq:refnmodel}
\end{align}
with the target reference is set to be an oscillating wave: $y_t = \cos(0.2 \pi t + 0.3)$. This toy example is first introduced in \cite{NK11}. It serves two purposes here. Firstly, it provides a simple example to verify our implementation\footnote{Strictly speaking, testing increases confidence but  does not prove no bug, which is almost impossible in practice.} Secondly, it severs as benchmark for the following experiments in which we attempt to use more complicated reference signals and models.

Setting the maximum time period, $T=50$, 

We proceed by examining the algorithm for the following different implementations: (a) qn = fn without
using the MCMC move (Step 2(d)), (b) qn = fn with the MCMC move and (c) qn being the optimal importance density of [13] without the MCMC case. In the last case the MCMC step was omitted because when the optimal importance density is used the improvement in performance was marginal. In the MCMC move we will use a random walk proposal. For  = 100, 1000 and N = 200, 500, 1000, 5000, 10000 we present box plots for
log⇡T(UI1:T)inFigure1after30independentrunsofthealgorithm,whereUI1:T istheestimatorofu⇤1:T ineach run. Similarly, in Figure 2 we plot U I1:T and the particle population nU i1:T oNi=1 taken from one run of the each
￼￼￼￼of the same cases, but this time we show results only for N = 10000. Simulations took roughly 3, 70 and 4 seconds per 1000 particles for (a), (b), (c) respectively when implemented in Matlab using a 2.4 GHz processor. The algorithm seems to perform quite well in most settings and very well when the optimal importance distribution is used. For the case where qn = fn, MCMC seems to improve the performance of the algorithm.
The improvement is more evident when  = 1000 both in the box plots and when plotting nUi1:T oNi=1, for which the degeneracy is apparent without the MCMC step.

\subsection{Period length performance}
We extend the time step to be $90$ and $250$ steps and looks at the corresponding performance. Due to the increase of time step, it makes sense to have more particles to track them and also look at the performance of performance of different order of $\gamma$. Based on the same metric, the results are summarised in Figure X.

\subsection{Increasing the power}

\section{Different reference signals}
Given the initial result looks promising, we attempt to investigate with the following more complicated reference signals:
\begin{enumerate}
\item reference signal treding osccilating wave ---
\item two un-correlated bi-variate signals ---
\item two correlated bi-variate signals ---
\item ten different signals ---
\end{enumerate}

\section{Discussion and possible extension}

\endinput


\section{Evidence for the thesis and future work}
\label{sec:5.eftt}
Formulating an optimal security policy is difficult. Current research
work attempts to alleviate this issue by looking for ways to analyse
and refine security policies in a top-down manner. We propose an
alternative view on this issue: inferring security policies from
decision examples. This idea is entirely novel. There is no previous
work to my knowledge in the application of EAs or machine learning
techniques in inferring security policies.

In this chapter, we presents some experiments that have been carried
out to validate this proposal using EAs. Three different ways of
representing security policies and the use of two different EAs are
demonstrated. The results show that the inference process is largely
independent of many parameters. We also show how the fuzzy set
ensemble based approaches can be easily integrated into the policy
inference framework to enhance the inference ability, yet it remains
an interesting research topic to search for the optimal ways of
defining the underlying target fuzzy membership functions.

EAs have shown several potentials in determining the security policies
in the experiments. In particular, EAs are found to be able to quickly
infer security policies with considerable complexity. The performance
of these inferred policies is comparable to the original reference
models that are used to generate the training sets. These techniques
are also able to scale well with the range of input/output variables
and to tolerate~``wrong'' examples in a training set. An obvious way
forward is to validate this concept with other inference approaches
and make a recommendation on which approach is better for what
circumstance.

Being a data driven approach, the representativeness of the training
set is crucial. Indeed, the experiments show that even the inference
of the simple MLS Bell-LaPadula model may fail because of
this. Inference summarises rather than speculates; the techniques do
not know how to handle an unseen case.

As in other applications of EAs, the fitness function used is vital in
guiding the search. Poor fitness function may result in policies that
are suboptimal. Interesting future work would be to examine how to
design a fitness function in a principled manner that is suitable for
cost sensitive learning, in which different types of prediction errors
are not equally costly. This is likely to be appropriate in security
policy in which leaking of high sensitivity information is obviously
far more severe than leaking of low sensitivity information.

\section{Conclusions}
\label{sec:conclusion5}
This chapter presents some proof-of-concept experiments that have been
carried out to validate our proposal: inferring security policies from
decision examples using EAs. It first presents the experiments on
inferring some simple binary decision policies and continues with the
experiments on inferring the Fuzzy MLS model, which is a more
complicated multi-decision policy model. In all cases, the results
show that EAs are able to infer policies that can approximate (if not
refine) the original reference models that are used to generate the
training sets. The technique is also shown to be able to scale with
the range of input/output variables and to tolerate~``wrong'' examples
in the training set.

For a dynamic environment, the ability to infer policy from examples
alone is not sufficient. The inferred and learnt policies will
eventually become suboptimal over time as the operational requirements
change. The policy needs to be updated continually to maintain its
optimality. The next chapter demonstrates how multi-objective
evolutionary algorithms (MOEAs) can be used to achieve this goal.
