\chapter{Dynamic Security Policy Inference}
\graphicspath{{Chapter6/figures/}}
\label{DynamicSecurityPolicyInference}
Recent research has suggested that traditional top down security
policy models are too rigid to cope with changes in dynamic
operational environments. There is a need for greater flexibility in
security policies to protect information appropriately and yet still
satisfy operational needs. The previous chapter has shown that
security policies can be learnt from examples using machine learning
techniques. Given a set of criteria of concern, one can apply these
techniques to learn the policy that best fits the criteria. These
criteria can be expressed in terms of high level objectives, or
characterised by the set of previously seen decision
examples. Nevertheless, this is not sufficient because the learnt
policy will eventually become suboptimal over time as the operational
requirements change. The policy needs to be updated continually to
maintain its optimality.

This chapter proposes two dynamic security policy learning frameworks
based on Genetic Programming. This techique was shown to be able to
learn the security policy from examples in the previous chapter. The
motivations for using Genetic Programming remains the same: it makes
rather weak assumptions on the solution space and therefore has the
ability to search for solutions of unknown (or controlled) size and
shape in very large, discontinuous solution spaces. Other data mining
algorithms and heuristic search techniques are potentially applicable.

% The main contributions of this chapter include:
% \begin{enumerate}
% \item The design of a dynamic security policy model. Currently there
%   are no decision examples that are available for us to work
%   with. This model is used to generate examples for training and also
%   evaluation purposes.
% \item The discovery of problems and difficulties encountered in
%   dynamic learning using Genetic Programming.
% \item The design of two dynamic learning frameworks using Multi
%   Objective Genetic Programming (MOGP). Both can be easily generalised
%   to work with other data mining algorithms.
% \end{enumerate}
The rest of this chapter is organized as follows: Section
\ref{sec:Policy} presents a time-varying risk budget based
policy. Section \ref{sec:streammining} reviews some data stream
classification algorithms that provided the inspiration for the
development of our dynamic policy learning frameworks. Section
\ref{sec:ProblemRepresentation} presents the general experimental
setup. Sections \ref{sec:StaticLearning} and \ref{sec:DynamicLearning}
present the experimental details and results on static and dynamic
security policy learning respectively. Section
\ref{sec:6.ModelSelection} presents the analysis of various ways to
select the best solution from a set of candidate solutions generated
by MOGP. Section \ref{sec:6.Conclusion} concludes and discusses future
research.

\section{A Time-varying, Risk Budget based Security Policy Model}
\label{sec:Policy}
Since there is no widely used security policy model that supports
time-varying policies, a time-varying risk budget based security
policy model is first designed. This model has two purposes:
generating training decision examples and serving as the benchmark
against which the security policies learnt are evaluated.

In a system using a risk budget based security policy, each user
is given an amount of risk tokens that represents how much risk
the system is willing to take with that user. To access a piece of
information, a user offers the amount of risk tokens he is willing
to spend from his budget to pay for the access. The system
evaluates the risk incurred in granting the access and allows it
only if the user's offer is greater than or equal to the risk. The
risk evaluation is part of the policy and may change with time.

The definition of risk in Fuzzy MLS model in \eqref{eq:risk} is reused
here and is restated as follows for ease of referencing:
\begin{equation*}
  \text{risk} = \text{value of damage, } V \times \text{probability of incurring the damage, } P 
\end{equation*}

The probability of incurring damage, $P$ is the joint probabilities of
these four \emph{independent} probabilities:
\begin{enumerate}
  \item $P_{CH}$: The probability that the communication channel between the user and the system is compromised.
  \item $P_{IS}$: The probability that the information system is compromised.
  \item $P_{HU}$: The probability that the human user is compromised, i.e., being tempted, malicious, careless, etc.
  \item $P_{PH}$: The probability that the physical security of the user or the system is compromised.
\end{enumerate}
It should be noted that $P_{CH}$, $P_{IS}$ and $P_{HU}$ exclude the
probability of physical compromises that are covered by $P_{PH}$. The
independence assumption among these probabilities may not hold and
results in $P$ being over estimated. This is fine from the security
perspective, especially given the fact that all these probabilities
are only estimates to begin with.

To estimate $P_{CH}$, we consider the security levels of communication
channel, $S_{CH}$ may be either secure ($S_{CH} = 1$) or not ($S_{CH}
= 0$). $P_{CH} = 0$ only if $S_{CH} = 1$ and $P_{CH} = 1$ otherwise.

To estimate $P_{IS}$, we submit to the five information system
security rating levels, $S_{IS}$ as outlined in Trusted Computer
System Evaluation Criteria (TCSEC) \cite{DOD85}. We assume that
$S_{IS}$ is an integer in $[0, 4]$; the higher $S_{IS}$, the more
secure the system is. $S_{IS}$ is mapped to $P_{IS}$ using an inverse
exponential function such that $P_{IS} = 1/\exp(S_{IS})$.

To estimate $P_{HU}$, we consider the sensitivity levels of the
subject (user), $sl$ and the object (information), $ol$. The
sensitivity level of a subject represents the level of trustworthiness
of the subject whereas the sensitivity level of an object indicates
the level of damage incurred if the object is lost or misused.  To map
these sensitivity levels to $P_{HU}$, the sigmoid function in
\eqref{eq:P1} is reused and restated here as follows:
\begin{equation}
P_{HU} = \frac{1}{1 + \exp(-k(TI(sl, ol) - mid))}
\end{equation}
where $TI(sl, ol)$ is the temptation index which indicates how much
the subject with sensitivity level $sl$ is tempted to leak information
with sensitivity level $ol$. $TI$ in \eqref{eq:TI} is reused
and restated here as follows:
\begin{equation}
TI(sl, ol) = \frac{a^{(ol - sl)}}{M - ol}
\end{equation}
The intuition for $P_{HU}$ and $TI$ can be found in Section
\ref{FuzzyMLSModel}. In our experiments, the settings used in the
experiments presented in previous chapter is reused: $sl$ and $ol$ are
integers in $[0, 9]$, $k = 3$, $mid = 4$ and $M = 11$.

To estimate $P_{PH}$, we assume there are $10$ physical security
rating levels, $S_{PH}$ which takes an integer in $[0, 9]$. Higher
rating level indicates better physical security protection mechanisms
are in place. The mapping function from $S_{PH}$ to $P_{PH}$ is
$P_{PH} = (9 - S_{PH}) / 9$.

The value of damage, $V$ may be estimated using an exponential
function such that $V = a^{ol}$. The setting $a = 10$ used in the
experiments presented in previous chapter is reused here.

To introduce dynamic changes to the security policy, the risk
calculated is multiplied by a safety margin factor, $\alpha$ which has
a value that varies over time in accordance with the changing
environment. For example, the security policy may be more restrictive
and has a larger value of $\alpha$ at night. The value of $\alpha$ is
restricted here to be a real value in $[1.0, 3.0)$.  The
\emph{evaluated risk} for an access to a piece of information
therefore becomes $\alpha \times P \times V$.

We make the assumption that a user is rational in making each access
request to information in the following ways. Firstly, the user is
able to estimate the risk associated with the access to a certain
degree of accuracy. Secondly, the user always attempts to minimise the
amount of risk tokens spent on the access without generating too many
responses that result in a denial of access.

To model this, we assume each user always makes an offer of
$(\beta_{min} + \gamma) \times P \times V$, where $\gamma$ is a random
variable with a beta distribution which has a mean value of $0.5$ and
a variance value of $0.05$. The user adjusts its $\beta_{min}$ over
time based on the allow/deny responses he receives by using a
counter. The counter is incremented if an access request is granted or
decrements otherwise. After every $5$ decisions, the user increases
$\beta_{min}$ by $0.1$ if the counter value is positive or decreases
$\beta_{min}$ by $0.1$ otherwise. The counter is then reset to
zero. The value of $\beta_{min}$ is initialised to be $0.5$ less than
the initial value of $\alpha$.

Examples of access control decisions are generated using the setting
described above.  Each example is a tuple of $S_{CH}$, $S_{IS}$, $sl$,
$ol$, $S_{PH}$, $\mathit{offer}$ and $\mathit{decision}$. A set of
$10000$ examples is generated, the value of $\alpha$ is changed
randomly within its range after every $1000$ examples.  GP/MOGP is
used to learn the underlying security policy model from these
examples.

\section{Data Stream Classification}
\label{sec:streammining}
This section reviews some data stream classification algorithms that
provided the inspiration for the development of our policy learning
framework. See \cite{MMG05, CCA06} for details on classification
algorithms in data stream mining.

Data stream classification poses two challenges to the traditional
data classification algorithms: infinite data flow and concept
drifts. Infinite data flow prohibits a classification algorithm from
making multiple passes on the data set. Furthermore, the distribution
of the data changes over. This change is typically referred to as
\emph{concept drift}. The algorithms must be able to cope with this.

To do this, traditional algorithms have been revised to include fading
effects for the older examples.  A previously learnt classifier is
required to undergo revision and to relearn the new concept
constantly. Using the decision tree classifier as an example, the
decision tree/subtree is pruned, regrown or discarded as necessary
\cite{DP00}. The resulting algorithms are often complicated. Making
matters worse, as these algorithms discard old examples at a fixed
rate, the learnt classifier is only supported by the latest data. This
usually results in large prediction variances \cite{FW04}.

The ensemble of classifiers is another approach that has become very
popular in data stream classification. This approach offers several
advantages over single model classifier. Firstly, it offers an
efficient way to improve accuracy. Secondly, its parallel nature is
easy to scale. Often, the ensembles approach splits the learning
process into 2 parts: data summarisation and selection of models. Data
in the stream is divided into chunks; a classifier ensemble (which
itself can also be a set of classifier ensembles) is built from each
data chunk. These classifier ensembles are combined using different
weights to form the ultimate classifier.  Some of the criteria used to
determine the weights include the time (sliding window),
\emph{estimated} accuracy using the latest data chunk and variation in
the class distribution.

Dynamic security policy learning is very similar to data stream
classification. Each possible decision in the policy can be viewed as
a class; the learning objective is to search for the classifier that
best agrees with the examples in the data stream in a timely
manner. In both cases, the amount of data will inevitably increase
over time, the algorithm must be able to learn incrementally and cope
with changes. Yet there is still a small distinction in terms of the
learning time requirement. In dynamic security policy learning, the
learning time requirement is much relaxed. Acceptable time frames
range from a few minutes to hours. This frees us from the one pass
data set constraint in data stream mining, i.e., old data can be
revisited if necessary.

One possible approach to this problem is to apply a static learning
algorithm only on the latest data and discard the old data
indiscriminately after a fixed time period. Whilst this approach is
conceptually simple, the model learnt in such an approach will
inevitably lack behind in time.

% The model is no longer predictive, but instead, has to chase the
% changes at all times.

In \cite{FW04} Fan presents a simple example to illustrate that old
data which are consistent with the new concept can help
learning. Instead of throwing away all the old data, he proposed a
framework that dynamically selects one of the following four
classifiers as the final classifier:
\begin{enumerate}
\item The optimal classifier trained so far without the use of latest
  chunk of data.
\item The classifier trained by updating the optimal classifier in 1
  with the latest chunk of data.
\item The classifier trained only with the latest chunk of data.
\item The classifier trained from scratch with the latest chunk of
  data and some old data samples that are (assumed to be) consistent
  with the latest chunk of data.
\end{enumerate}
The final classifier chosen is the one with the highest accuracy on
the latest chunk of data using cross validation. This classifier is
then set to be the optimal classifier trained so far (Classifier 1) in
the next classifier selection process.

Fan's intuition is that no one knows if the latest data chunk on its
own is sufficient to train a better classifier than the previous one
learnt. Instead of statically defining how much old data is to be
used, Fan's framework lets the data make the decision dynamically.

The design of our dynamic security policy learning framework begins
with this intuition and shows how MOGP can serve as an elegant
framework for dynamic learning. A novel idea is then proposed to
further reduce the error rate and response time to concept drifts.

\section{General Experimental Setup}
\label{sec:ProblemRepresentation} 
Multiple experiments on learning the dynamic security policy model
presented in Section \ref{sec:Policy} from decision examples using GP
have been carried out. The security policy model can be viewed as a
function that maps $6$ decision-making factors, $S_{CH}$, $S_{IS}$,
$sl$, $ol$, $S_{PH}$, $\mathit{offer}$ to a binary
$\mathit{decision}$. To use GP to search for a security policy, each
individual tree in the population is used to represent a security
policy candidate. The words ``individual'' and ``policy'' are used
interchangeably in the rest of this chapter.  The terminal (leaf)
nodes can be one of the decision-making factors or an Ephemeral Random
Constant (ERC)\footnote{An Ephemeral Random Constant (ERC) is a
  constant whose value is randomly generated during its creation.},
which takes a real value in $[-10.0, 10.0]$. The non-terminal
(non-leaf) nodes are mathematical functions. The functions chosen are:
$+$, $-$, $\times$,
$\div$\footnote{$x \div y~=~\begin{cases}x/y & \mbox{if } y \neq 0 \\
    1 & \mbox{otherwise}\end{cases}$} ,
$protectedln(x)$\footnote{$protectedln(x)~=~\begin{cases}ln(|x|)&
    \mbox{if } x \neq 0 \\ 0 & \mbox{otherwise}\end{cases}$},
$\exp(x)$, $pow(x,y)$,
$protectedlog_{x}(y)$\footnote{$protectedlog_{x}(y)~=~\begin{cases}log_{(|x|)}(|y|)&
    \mbox{if } x \neq 0 \mbox{ or } 1 \mbox{ or } y \neq 0 \mbox{ or }
    1 \\ 0 & \mbox{otherwise}\end{cases}$}, $\max(x, y)$, $\min(x,
y)$, $\sin(x)$ and $\cos(x)$.

In each experiment, a security policy is learnt and refined
continually using a set of decision examples. Ideally, the learnt
policy should generate the same decisions prescribed by all the
training examples. The practical objective is to minimise the
percentage of output decisions that are different from the ones
generated by the true model. This percentage is the \emph{error rate}
of the learnt security policy. This error rate is estimated using the
$1000$ examples generated from the same policy.

All the experiments are carried out using ECJ 18 \cite{ECJ97} with the
SPEA2 module obtained from the ECJ 19 CVS repository
\url{http://dev.java.net/}\footnote{There is a major revision on the
  SPEA2 module in version 19 to improve clarity and to remove some
  minor bugs found}. Unless otherwise specified, the default
parameters\footnote{\texttt{koza.params} for Single Objective GP and
  \texttt{spea2.params} for MOGP} were used in all experiments.

GP (and EAs in general) is stochastic in nature; the evolution process
in each run may vary depending on the random seed used. To evaluate
the performance, each experiment is repeated $100$ times with
different random seeds. The performance is evaluated by two criteria:
\begin{enumerate}
\item The median error rate of the best individuals in the $100$
  runs. The best individual in a run is the one with the lowest error
  rate in the last generation of the run. The median is used instead
  of mean as it does not assume that the error rate distribution will
  have suitably normal distribution. This follows that the confidence
  interval based on the standard deviation of mean is no longer
  valid. The $95\%$ confidence interval of the median is calculated
  using the Thompson-Savur formula presented in \cite{MH73} instead.
\item The number of the best individuals with error rates $\leq
  0.25$. What's the rationale of choosing this value?
\end{enumerate}
These measurements indicate the quality of the security policy that
can be learnt and the likelihood of learning a reasonably good
security policy using GP.

\section{Static Policy Learning}
\label{sec:StaticLearning}
To prepare for the experiments on dynamic policy learning, we started
with three experiments to learn static policy. The training set used
was the first $1000$ examples generated by the model as described in
Section \ref{sec:Policy}. The policy to be learnt is static in the
sense that all $1000$ examples are generated with the same value of
$\alpha$. Experiment $1$ uses single objective GP. Experiment $2$ and
Experiment $3$ use MOGP-SPEA2 to address the problems encountered in
Experiment $1$.

\subsection{Experiment 1: Single-Objective Genetic Programming}
In Experiment $1$, the Single-Objective Genetic Programming and the
default genetic operators and parameters are used. Each selected
individual (policy) has a probability of $0.9$ of being subjected to a
crossover operation and a probability of $0.1$ of being reproduced.
The binary tournament selection scheme \cite{AB81} is used to select
the individuals from the current population to breed the next
generation individuals. This scheme holds several tournaments, with
each tournament randomly choosing two individuals from the current
population and then selecting the fitter one of the two. The
individual fitness is its error rate. Let $r_{i}$ be the $6$
decision-making factors and $\mathit{decision}_{i}$ be the decision
encoded in example $i$, and $P(r_{i})$ be the decision of a policy $P$
on $r_{i}$, then the fitness function of $P$ is:
\begin{equation}
  f_{all}(P)~=~\frac{1}{n}~\sum^{n}_{i=1} \left ( P(r_{i})\neq decision_{i}
  \right )
\end{equation}
In this case, $n = 1000$ and the values $1$ and $0$ are used to
represent Boolean values $\mathit{True}$ and $\mathit{False}$
respectively.

The experiment is repeated $100$ times. Each experiment runs for $200$
generations. The median error rate of the $100$ best individuals is
$0.3555$ with its $95\%$ confidence interval of $[0.3487,
0.3640]$. The best of the $100$ best individuals has an error rate of
only $0.107$. However, there are only $12$ out of the $100$ best
individuals, who have error rates of $\leq 0.25$ and more than half of
them have error rates of $> 0.35$. This suggests that many runs got
stuck at local optimums.

Analysis on the structures of the individuals in the population
reveals some common problems in GP. The average individual size
(number of nodes) in the population grows quickly and becomes very
large. This can be a phenomenon of bloat (uncontrolled growth of the
average size of the individuals), overfitting problems or both. No
attempt is made to distinguish them here as both make the learning
process more difficult, use more memory and require more evaluation
time.

\subsection{Experiment 2:  Bloat Control with SPEA2}
In Experiment $2$, the SPEA2 bloat control method \cite{SB01} is used
to alleviate these problems. This method introduces a new objective
--- minimising the individual size (the number of nodes in the
individual tree). Let $size(P)$ be the size of an individual $P$, the
fitness function for this new objective is:
\begin{equation}
f_{size}(P)=
\begin{cases}
size(P)/512 & \mbox{if } 32 \leq size(P) \leq 512
\\
32/512      & \mbox{if } size(P) < 32
\\
1           & \mbox{otherwise}
\end{cases}
\end{equation}
In other words, individuals with less than $32$ nodes have the same
$f_{size}$ values as the individual with $32$ nodes and individuals
with more than $512$ nodes have the same $f_{size}$ values as the
individual with $512$ nodes. This is to avoid both over simplified and
over complicated solutions.

SPEA2 is an elitist approach. An archive is maintained so that the
non-dominated individuals of a generation can be preserved in the
archive and passed on to the following generation. Therefore the
reproduction operator is removed, i.e., the crossover operator is
applied with probability of $1.0$.

The experiment is carried out with these changes. The results show
that the bloat control method is effective. The average individual
size, memory required and evaluation time taken are reduced
significantly. However, the performance improvement in terms of error
rate is marginal. The median error rate of the $100$ best individuals
is $0.3545$ with its $95\%$ confidence interval of $[0.3419,
0.3591]$. The best of the $100$ best individuals has an error rate of
only $0.126$ and there are only $22$ best individuals that have error
rates of $\leq 0.25$.

It has also been found that the diversity among constants appearing in
the individuals decreases with each new generation. This is expected
as some form of convergence is necessary if a population is to produce
a solution for the problem in question. It is often that the desired
constants will not appear in the initial population that are randomly
generated; but the evolutionary process is expected to synthesise the
required constants by joining the existing ones through the
operators. If the constants converge prematurely before finding the
appropriate ones, the evolutionary process may be stuck at a local
optimum.

This lack of diversity among constants is the key to our problem. This
is revealed by analysis of the risk based policy discussed in Section
\ref{sec:Policy}. This model can be written as:
\begin{align}
\mathit{allow~iff~offer} &\geq \mathit{risk} \nonumber \\
\Rightarrow~\mathit{allow~iff~offer} &\geq \alpha \times P \times V \nonumber \\
\Rightarrow~\mathit{allow~iff~offer} &\geq \alpha \times P \times a^{ol} \label{eqn:a_pol}
\end{align}
As $P$ is in $[0.0, 1.0]$ and $\alpha$ is in $[0.0,3.0)$ and
$\mathit{offer}$ is programmed to track $\mathit{risk}$, the value of
constant $a$ which is raised exponentially dominates the right hand
side of \eqref{eqn:a_pol}. The error rate of an individual
would remain to be high even if it implements the exact inequality
except with a wrong value of $a$. As the diversity of the constants
decreases over generations, the chance of finding the correct value of
$a$ becomes even smaller.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Moreover, the rate of convergence among constants in our experiments
is further accelerated by the following factors:
\begin{itemize}
\item Small chance of using a constant as a leaf node --- In ECJ, each
  ERC constant of a range is implemented as a terminal class and each
  variable is implemented as a terminal class.  Each terminal class
  has an equal probability of being selected to be a leaf node. As our
  experiments consist of $6$ variable terminal classes and $1$ ERC
  terminal class, the probability of an ERC to be chosen as a leaf
  node is only $1/7$. Furthermore, the relatively large ERC range also
  exacerbated the problem in finding the appropriate constant.
\item No mutation operator --- Without mutation, there is no new
  constant introduced to the population at all. The diversity among
  constants in the population decreases with each new generation.
\item The use of an archive in SPEA2 --- SPEA2 restricts the binary
  tournament selection to be among the individuals in the archive and
  therefore only the constants that appear in these individuals have
  the opportunity to be passed to the next generation.
\end{itemize}

Further analysis also reveals that substantial portion of the
individuals in the archive share the same or similar higher level
structures of their trees (assuming the root of the tree is the
highest node). The diversity among the individuals is lost. We think
the cause of this problem is similar: the absence of mutation and the
use of an archive in SPEA2.

\subsection{Experiment 3: Enhanced Constant Creation and Evolution}
To overcome these problems, Experiment 3 is setup with the following
changes. Firstly, $6$ identical ERC classes are used to make the
probabilities of choosing a variable and a constant to be
equal. Secondly, a mutation only setting is used instead of the typical
high crossover and low mutation setting. There are four reasons to do
so. Firstly, mutation introduces new genes and therefore diversifies the
population. Secondly, much empirical evidence shows crossover provides
no advantage over mutation in GP. Thirdly, this setting frees us from
tuning the probability parameters of applying crossover and
mutation. Finally, mutation provides a means to introduce new
individuals by simply allowing mutation to happen at the root node of
an individual. The probabilities of applying mutation at the root
node, at a terminal node and at a non-terminal nodes are changed from
the $0$, $0.1$ and $0.9$ (default) to $0.125$, $0.125$ and $0.75$
respectively.  The value of $0.125$ is the ratio of the archive size
($128$) over the population size ($1024$) used.

The results improve significantly. The median error rate of the $100$
best individuals is reduced to $0.2225$ with its $95\%$ confidence
interval of $[0.1595, 0.2789]$. The best of the $100$ best individuals
has an error rate of only $0.099$ and the number of best individuals
with error rate $\leq 0.25$ is $54$.

The distributions of the best individuals in all $3$ experiments on
different error rate intervals are shown in Figure
\ref{fig:StaticDist}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{StaticDist}
\caption{The distributions of the best individuals in all $3$
  experiments on different error rate intervals.}
\label{fig:StaticDist}
\end{figure}

\section{Dynamic Policy Learning}
\label{sec:DynamicLearning}
So far, it is assumed that all the decision examples are available
prior to the start of the learning process. This section investigates
how to learn security policies dynamically when new decision examples
become available gradually during the course of learning.  The learnt
model is continuously refined or even redefined if necessary by these
new examples. This section presents $3$ experiments: Experiment $4$
shows how dynamic learning can be performed by extending the current
framework using Fan's intuition and Experiments $5$ and $6$ use a
novel dynamic learning framework --- Diversity via Opposite Objective
(DOO).

Unlike before, the decision examples are organized into a sequence of
data chunks. Each chunk consists of $200$ examples. The policy changes
every $5$ chunks by changing its value of $\alpha$. In each
experiment, the sequence is fed into the learning framework one chunk
at a time. The first policy is learnt using the first chunk after
$100$ generations of evolution. Then, each subsequent chunk is used to
refine the policy learnt from the previous chunks. Each refinement
starts with the population of the last generation learnt from the
previous chunks, and uses the examples in the latest chunk to learn a
refined policy after a further $100$ generations of evolution.

The learning problem is divided into two subproblems: data
summarisation (model construction) and the optimal model
selection. This section only focuses on data summarisation problem and
simply assumes that the output model is the one with the lowest error
rate on the latest chunk of examples. The optimal model selection
problem will be addressed in the next section.

\subsection{Experiment 4: Fan's Intuition}
In Experiment $4$, the current framework is extended in $2$
ways. Firstly, the number of examples, $n$ in $f_{all}$ is no longer
fixed to $1000$ but set to be the total number of examples received
and therefore increases as more chunks are received. Secondly, a new
fitness function is introduced to measure the error rate of a policy
with respect to the latest chunk of data. Let $r_{i}$ be the $6$
decision-making factors and $\mathit{decision}_{i}$ be the decision in
example $i$, $s$ be the size of a data chunk, and $P(r_{i})$ be the
decision made by policy $P$ on $r_{i}$, this new fitness function is:
\begin{equation}
  f_{last}(P) = \frac{1}{s} \sum^{n}_{i=n-s+1} \left ( P(r_{i})\neq
  decision_{i} \right )
\end{equation}
Along with this new fitness function, this experiment has $3$ fitness
functions in total, namely $f_{all}$, $f_{last}$ and $f_{size}$.

The intuition employed here is similar to Fan's (cf. Section
\ref{sec:streammining}). Each chunk of examples is used to refine the
policy learnt from the previous chunks.  The refinement process
happens through $100$ generations of evolution. In each generation,
the policies with the lowest $f_{all}$ or $f_{last}$ values in the
previous generation are preserved in the SPEA2 archive. These policies
correspond to Fan's Classifier 1. These policies are then refined with
examples in the latest chunk through evolutionary process. These
refined policies correspond to Fan's Classifier 2. New policies are
generated (using mutation). Some will have the lowest $f_{last}$
values within the population and correspond to Fan's Classifier
3. Some others will have the lowest $f_{all}$ values and correspond to
Fan's Classifier 4.  After $100$ generations, the policy with the
lowest $f_{last}$ value is chosen to be the new learnt policy.

The results are shown in $2$ resolution levels. Figure
\ref{fig:exp5summary} shows the median error rate of the best policies
learnt after every $100$ generations of training time. Except in the
initial $500$ generations, the median error rate is kept $\leq 0.220$
at all times. To show the learning progress in detail, Figure
\ref{fig:exp5} shows the median error rate of the best policies learnt
in each generation.

\begin{figure}[htbp]
  \centering \subfigure[The median error rate of the best policies
  learnt after $100$ generations training
  time]{\includegraphics[width=0.475\textwidth]{newexp5summary}
    \label{fig:exp5summary}
  } \subfigure[The median error rate of the best policies learnt in
  each generation]{\includegraphics[width=0.475\textwidth]{newexp5}
    \label{fig:exp5}
  }
  \caption{The median error rate of the best policies learnt in
    Experiment $4$.}
\end{figure}

When a policy change happens at every $5$ chunks, i.e., $5 \times 100
= 500$ generations, the median error rate spikes up sharply. After the
spike, the error rate decreases faster than the initial $500$
generations. This is a direct effect of using the $2$ fitness
functions: $f_{all}$ and $f_{last}$ together. This setting provides a
smooth transition phase for learning. It protects the individuals that
are optimal with respect to the old policy from being eliminated too
quickly and therefore allows these individuals have chances to pass on
the knowledge they learnt to the new individuals. Consider the
population in the generation prior to the change, the individuals that
have low $f_{last}$ values are also likely to have low $f_{all}$
values. After the change, their $f_{last}$ will become worse (higher),
but their $f_{all}$ values would only be affected slightly. Thus,
these individuals will still have a good chance to be kept in the
archive and be brought forward to the next generation.

% In our problem, as a policy change is characterised by a change on the
% value of $\alpha$, learning a new policy is thus reduced to learning a
% new value for $\alpha$. This is obviously simpler than learning from
% scratch.

Analysis also reveals the individuals in the archive converge and
become more alike to one another over generations. This is not an
issue in this problem provided the ``structure'' of the policy is
learnt before the diversity is lost. The subsequent changes only
require changes on the value of $\alpha$ which mutation can easily
provide. The loss of diversity also explains the very sharp upward
spikes in the error rate during policy change as shown in Figure
\ref{fig:exp5}. As the individuals are all alike, none of them would
match the new policy well and thus the error rate increases
sharply. However, the new policy is relatively easy to learn and thus
the quick decrease in the error rate.

In summary, this approach can perform well under the assumption that
the policy changes are relatively small and the knowledge learnt
previously aids the learning process of the new policy. The use of $2$
fitness functions: $f_{all}$ and $f_{last}$ protects and allows the
knowledge learnt to be passed from generation to generation. However,
this approach still suffers from the loss of diversity among the
individuals in the population. This puts the general applicability of
this approach in question.

\subsection{Diversity via Opposite Objectives (DOO)}
The loss of diversity problem is not uncommon in EAs. The ``survival
of the fittest'' principle in EAs provides the fitter individuals
higher chances to survive and pass their genes to the individuals in
the next generations. Consequently, the individuals in the population
will inevitably become more alike with one another over generations.
If significant diversity is lost prior to the optimal individual being
found, the population is said to be converged prematurely to a local
optimum. To prevent this, EAs use mutation operators to introduce new
random genes and individuals. However, the chances that these new
random genes and individuals would provide improvement over current
individuals are very small and therefore they are highly unlikely to
be preserved. In other words, diversity is generated and then lost
from one generation to the next.

To overcome this problem, several dynamic learning algorithms based on
EAs have been proposed in the literature. Most, if not all of them
\emph{first} attempt to produce individuals that are optimised for the
problem-related objectives and \emph{then} attempt to maintain the
diversity among individuals in the population as much as possible
\cite{BJ01}. Their settings are often ad-hoc and the algorithms are
often complicated. A new dynamic learning framework --- Diversity via
Opposite Objectives (DOO) is proposed here. DOO takes the opposing
perspective; it \emph{first} attempts to maximise the diversity among
individuals in the population through generations and \emph{then} uses
the ever increasing diversity to help in finding the optimised
individuals.

In EAs, performing evolutionary operations on a single individual can
be viewed as searching for more optimised individuals from the
position of the individual in the solution space. Similarly,
performing evolutionary operations on a diverse population of
individuals can be viewed as searching many different parts of the
solution space in parallel. This parallel search has a much better
chance of finding more optimised individuals than a search starting
from just one individual. The ever increasing diversity in DOO results
in a domino effect such that not only the search is done in parallel,
but the search space coverage increases as the diversity
increases. Consequently, the chance of finding a more optimal
individual becomes higher and higher as the evolution proceeds. This
same effect cannot be achieved by conducting many parellel
single-objective EA runs because each run is likely to be trapped in a
local optimum.

In DOO, each and every objective is changed into a pair of opposing
objectives. For example, the objective of minimising error rate in our
problem is changed to minimising error rate and minimising accuracy
($1.0 - \mathit{error~rate}$). DOO then optimises all objectives using
MOEAs. The opposing objectives in DOO ensures that an individual who
is weaker for one objective is fitter for the opposite
objective. Therefore, \emph{no individual is dominated by others},
i.e., all individuals are at the Pareto Front. Each and every
individual has a fair chance of passing its genes to the next
generation. This prevents the population from convergence, i.e.,
diversity is at least maintained. Since the true Pareto Front is
already found at the start of the evolution process, the only job left
for MOEAs is to improve the spread of the solutions on the Pareto
Front. As a result, the population of a generation would have a wider
coverage of the solution space than the previous generations, i.e.,
diversity increases as MOEAs drive the evolution process.

To understand how DOO works via MOEAs, the concepts of the solution
space, $S$ and objective space, $O$ introduced in Section \ref{MOEA}
are used. Referring to Figure \ref{fig:mapping2}, each point $s \in S$
represents a possible solution to the problem in question. The fitness
function, $f$ maps a point $s \in S$ to a point $o \in O$ such that
the location of the point $o$ represents how well $s$ meets the
objectives. The function $f$ is surjective (onto) but not injective
($1$ to $1$); each point in $S$ is mapped to a point in $O$ and
multiple points in $S$ can be mapped to one point in $O$.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{mapping}
\caption{The mapping between solution space, $S$ and objective space, $O$.}
\label{fig:mapping2}
\end{figure}

Diversity among individuals in a population is essentially a measure
on how uniformly the solution points are distributed in $S$. While $f$
does not conserve the distribution, it is often true in many problems
that $f$ is a continuous function between these two topological
spaces, i.e., a set of points near a point $s'$ in $S$ is mapped to a
set of points near the point $f(s')$ in $O$.

As $f$ is not injective, the inverse is not necessarily true, e.g.,
2 solutions can be very different yet both solve the same problem
equally good. However, following the continuity assumption
on $f$, it is reasonable to assume that a set of points in $O$
that are far apart from one another corresponds to a set of points
that are far apart from one another in $S$. DOO makes use of this
assumption and attempts to maximise the diversity of solution
points in $S$ via maximising the diversity of points in $O$ using
MOEAs.

In the SPEA2 implementation of MOEAs, if the number of non-dominated
points exceeds the archive size, the point that has the shortest
Euclidean distance to another individual in the objective space is
dropped. If two solutions have the same distance to their nearest
neighbours, the tie is broken by comparing their distances to their
second nearest neighbours and so forth. This process is iterated until
the non-dominated solutions can fit into the archive.  Essentially,
the goal is to fill the archive with non-dominated points as uniformly
and widely distributed over $O$ as possible. Using the $\langle$error
rate, accuracy$\rangle$ pair of objectives as an example, a possible
Pareto Front and points that are to be kept in the archive is depicted
in Figure \ref{fig:DOO2}. As the optimal point with respect to each
objective is located at the corner of the Pareto Front, i.e., they are
furthest apart from other points, they are guaranteed to be preserved
at each generation until a better one is found. At the same time, a
better point is more likely to be found as the diversity keeps
increasing.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\textwidth]{DOO}
\caption{The setting of opposing objectives implies that all
  individuals are at the Pareto front. SPEA2 truncates its archive
  such that individuals that are close together are removed
  \emph{iteratively} until they can fit into the archive. Assuming the
  archive size is $10$, the individuals removed are illustrated. The
  individuals at each corner are guaranteed to survive from this
  archive truncation process.}
\label{fig:DOO2}
\end{figure}

Furthermore, all binary decision policies (all binary classifiers in
general) can be inverted to their complements by a simple negation on
their output decisions.  Therefore, high error rate policies are just
as good as those with the low error rate. To gain benefit from this,
DOO selects the policy with the highest absolute value of bias which
is defined as $0.5 - \mathit{error~rate}$.  A negative bias value
implies that the policy is optimised on the opposite objective and
thus all its output decisions need to be negated if it is selected for
use.

\subsection{Experiment 5: Two Pairs of Opposing Objectives}
Experiment $5$ uses two pairs of opposing objectives: $\langle
f_{all}$, $1.0 - f_{all} \rangle$ and $\langle f_{last}$, $1.0 -
f_{last}\rangle$. $f_{size}$ is excluded here as it is not a
problem-related objective. All other settings remain the same as in
Experiment $4$.

Figure \ref{fig:exp5asummary} shows the median error rate of the best
policies learnt after every $100$ generations of training time. Figure
\ref{fig:exp5a} shows the median error rate of the best policies
learnt in each generation. The results of Experiment $4$ that uses $3$
objectives are also included in pink for comparison purpose.

In the initial $500$ generations, the learning rate using DOO is
significant faster. The median error rate of the best policies is
reduced to $0.250$ in $200$ generations. This result is comparable to
the best static policy learning approach presented in Section
\ref{sec:StaticLearning}, despite the amount of training examples used
here being significantly less ($400$ vs. $1000$) and these examples
are presented to the learning algorithm in sequential chunks.

Furthermore, the median error rate of the best policies learnt in each
generation is lower than the one in Experiment $4$. However, the error
rates of the best policies may rise suddenly even in the absence of a
policy change. Analysis reveals that this is due to the way that the
output policy is selected in each generation. Currently, the output
policy in each generation is the one with lowest error rate/bias
\emph{estimated} using the latest $200$ examples. As these $200$
examples are generated randomly, they may not be sufficient in forming
a good representation of the target policy. Moreover, policy changes
sometimes may simply mean revisiting a past policy. Therefore, the
optimal policy in respect to the latest chunk may not be the true
optimal policy. This effect is not obvious in Experiment $4$ as all
the policy candidates (individuals) in the population are very
similar. In Section \ref{sec:6.ModelSelection}, we will show how the
ensemble approaches can be used to alleviate this problem. From
another perspective, this effect is a positive sign of diversity
maintenance.

Lastly, the heights of the spikes in the error rate due to policy
changes are much lower in DOO. This is a further evidence of
diversity maintenance. A policy change can be viewed as a change on
the fitness function, $f$. With a diverse set of policies
maintained in the population, it is likely that one of them will
be near the new target policy after the change.

\begin{figure}[htbp]
  \centering \subfigure[The median error rate of the best policies
  learnt after 100 generations of training
  time]{\includegraphics[width=0.475\textwidth]{newexp5asummary}
   \label{fig:exp5asummary}
 } \subfigure[The median error rate of the best policies learnt in
 each generation]{\includegraphics[width=0.475\textwidth]{newexp5a}
    \label{fig:exp5a}
  }
  \caption{The median error rates of the best policies learnt in
    Experiments $4$ (light pink) and $5$ (dark blue).}
\end{figure}

\subsection{Experiment 6: One Pair of Opposite Objectives}
An obvious weakness of Experiment $5$ is that it is not scalable. The
evaluation of $f_{all}$ involves scanning through all the decision
examples seen. As the number of examples increases over time, the
fitness evaluation time required for each individuals follows. A
possible way to overcome this is to use only a subset of decision
examples, randomly sampled from all the decision examples seen. The
likelihood of an example being sampled may also be set to decay over
time, i.e., the older an example is, the less likely it is being
sampled.

Prior to the search for a suitable sampling technique, it is always a
good idea to check if the old examples of any use. In Experiment
$6$, we drop the pair of objectives $\langle f_{all}, 1.0-f_{all}
\rangle$ from Experiment $5$ to see the effect of not evaluating
fitness against old decision examples.

The experiment is carried out with only this change. Figure
\ref{fig:exp6asummary} shows the median error rate of the best
policies learnt after every $100$ generations of training time and
Figure \ref{fig:exp6a} shows the median error rate of the best
policies in each generation. The results of Experiment $5$ that uses
$2$ pairs of opposing objectives are also included in pink for
comparison purpose.

\begin{figure}[htbp]
  \centering 
  \subfigure[The median error rate of the best policies
  learnt after 100 generations of training
  time]{\includegraphics[width=0.475\textwidth]{newexp6asummary}
    \label{fig:exp6asummary}
  }
  \subfigure[[The median error rate of the best policies learnt in
 each generation]{\includegraphics[width=0.475\textwidth]{newexp6a}
    \label{fig:exp6a}
  }
  \caption{The median error rates of the best policies learnt in
    Experiments $5$ (light pink) and $6$ (dark blue).}
\end{figure}

The experimental results show that the performance of the best
policies obtained in this experiment lay somewhere between those
obtained in Experiment $4$ and those obtained in Experiment $5$. This
suggests that the the pair of objectives $\langle f_{all}, 1.0-f_{all}
\rangle$ dropped are actually useful in maintaining a better diverse
set of individuals in the populations.

\section{Selection of Policy Models}
\label{sec:6.ModelSelection}
This section examines various ensemble approaches in which the
ultimate model is constructed by combining multiple models to achieve
better performance. We use a simple voting mechanism such that the
output of the ensemble is the majority output of all the models. This
ensemble construction comes at virtually no cost in EAs by simply
selecting the best $N$ individuals from the final population.

However, the theoretical study of ensemble approaches has revealed $2$
key factors that determine the performance of an ensemble are the
performance of individual models and the diversity among all models in
the ensemble \cite{AK95}. As the population of an EA run with non-DOO
setting converges and loses the diversity among individuals, the
performance gain of using ensemble is limited to the first
factor. However, this is not a problem with DOO. Yet, we still have
$2$ questions to answer:
\begin{enumerate}
\item How many models should be used to construct the ensemble? Whilst
  the negative bias models are as useful as the positive bias models,
  the models with near zero bias are virtually useless.  Should these
  models be included? If not, what should be the minimum threshold on
  the value of bias one model must have in order for it to be
  included?
\item Should the votes of all models have an equal weight? We choose
  to examine here if weighting the vote of each model with its bias
  using the latest chunk would provide better performance than a
  simple uniform weighted vote approach. If so, how much is the
  performance improvement?
\end{enumerate}

We attempt to answer these questions by comparing the performance of
ensembles built with the following combinations of models:
\begin{itemize}
  \item using the single highest bias model in the archive.
  \item using the $8$ highest bias models in the archive.
  \item using the $16$ highest bias models in the archive.
  \item using the $32$ highest bias models in the archive.
  \item using the $64$ highest bias models in the archive.
  \item using all the models ($128$) in the archive.
\end{itemize}
The bias of each model is estimated using the latest chunk of
examples.

The models in each of these ensembles are combined with $2$ different
methods: uniform weighted (unweighted) and bias weighted voting
mechanisms. In bias weighted voting mechanism, the vote of each model
is weighted with the absolute value of its bias on the latest data
chunk. If the bias is negative, its vote goes to the complement
decision class.

When using the ensemble approaches in Experiment $4$, the error rates
do not decrease but increase with the number of models used as shown
in Figures \ref{fig:exp5unweighted}--\ref{fig:exp5weighted}. This
is because not all the models are optimised with respect to the error
rate, some models in the archive are optimised with respect to other
objectives, e.g., the model size. When the number of models used is
small ($8$ or $16$ models), it is still very likely that the selected
models are those optimised with respect to the error rate. However, as
these models have converged to become very similar to one another, the
formed ensembles do not result in any performance gain nor loss. As
the number of models used increases, the ensembles begin to include
those models that are not optimised with respect to the error
rate. This causes the performances of the ensembles become worse. The
deterioration in performances is worse in the unweighted voting
mechanism.

The results on using the ensemble approaches in Experiment $5$ and $6$
are shown in Figures
\ref{fig:exp5aunweighted}--\ref{fig:exp6aweighted}. When the number of
models used is small ($8$ or $16$ models), there is no significant
change in performance. However, the error rate changes become
smoother, the sudden rises happen in the absence of policy changes
seem to disappear.  This smoothing effect is especially clear between
generations $2500$ to $3000$ and also between generations $3500$ to
$3700$. As the number of models used increases, the performance only
becomes slightly worse. The diversity maintained in DOO provides
performance gain to counter the performance loss due to the use of
less bias models in an ensemble.

\begin{figure}[htbp]
\centering
  \subfigure[Experiment 4: Unweighted voting]
  {\includegraphics[width=0.475\textwidth]{newexp5unweighted}
    \label{fig:exp5unweighted}
  }
  \subfigure[Experiment 4: Weighted voting]
  {\includegraphics[width=0.475\textwidth]{newexp5weighted}
    \label{fig:exp5weighted}
  }
  \subfigure[Experiment 5: Unweighted voting]
  {\includegraphics[width=0.475\textwidth]{newexp5aunweighted}
    \label{fig:exp5aunweighted}
  }
  \subfigure[Experiment 5: Weighted voting]
  {\includegraphics[width=0.475\textwidth]{newexp5aweighted}
    \label{fig:exp5aweighted}
  }
  \subfigure[Experiment 6: Unweighted voting]
  {\includegraphics[width=0.475\textwidth]{newexp6aunweighted}
    \label{fig:exp6aunweighted}
  }
  \subfigure[Experiment 6: Weighted voting]
  {\includegraphics[width=0.475\textwidth]{newexp6aweighted}
    \label{fig:exp6aweighted}
  }
  \subfigure{\includegraphics[width=0.95\textwidth]{legend}
    \label{fig:legend}
  }
  \caption{The median error rates of the best ensemble policies learnt
    in Experiments $4$, $5$ and $6$.}
\end{figure}

\section{Conclusion and Future Work}
\label{sec:6.Conclusion}
This chapter shows how evolutionary algorithms can be used to learn
dynamic security policies from examples. Two dynamic security policy
learning frameworks are proposed: one based on Fan's intuition and one
based on Diversity via Opposite Objectives (DOO). DOO treats an $N$
objectives optimisation problem as $2N$ objectives optimisation
problem by adding an opposing objective for each of the original
objectives. This allows the diversity in the population be maintained
whilst optimising the intended objectives. Diversity maintenance helps
in avoiding premature convergence and coping with concept drift in
dynamic learning.

The envisaged future work includes seeking for an appropriate sampling
techniques to select the old decision examples to make the DOO
framework scalable and also examining the generality of the DOO
framework further by using it with different individual
representations such as neural networks and decision trees to solve
other different problems.
