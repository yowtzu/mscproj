\chapter{Learning Techniques}
\graphicspath{{Chapter4/figures/}}
\label{LearningTechniques}
This chapter details the learning techniques used in this thesis. It
begins with a brief introduction to evolutionary algorithms (EAs) and
discusses two implementations of EAs, namely Genetic Programming (GP)
\cite{JRK89} and Grammatical Evolution (GE) \cite{CR98} that are used
in this thesis. It then discusses how EAs can be extended to solve
multi-objective optimisation (MOO) problems. The last section
introduces fuzzy expert systems. These are used as examples in Section
\ref{FuzzyPolicyEnsembles} to show how other learning techniques can
be used in conjunction with EAs to improve the learning performance.

\section{Evolutionary Algorithms (EAs)}
\label{EA}
Evolutionary Algorithms (EAs) are a set of heuristic search algorithms
inspired by natural selection\footnote{Loosely speaking natural
  selection states that individuals that are best adapted to the
  environment have better chance to survive and reproduce for next
  generation \cite{CD59}.}. An initial population of individuals,
which represents candidate solutions to the problem in question, is
generated. This is typically done in a random fashion or even better,
seeded with some known good solutions to provide better search
guidance. The individual in the population is then repeatedly
subjected to the evolutionary process that consists of the following
steps:
\begin{description}
\item [Evaluation] --- each individual is associated with a fitness
  value that measures how well each candidate solves the problem.
\item [Selection] --- individuals are selected for reproduction
  according to their fitnesses (natural selection). This implements
  the notion of ``survival of the fittest \cite{HS64}''.
\item [Reproduction] --- selected individuals are used to breed the
  population of the next generation using evolutionary operators. 
\end{description}
This evolutionary process produces populations of individuals
(candidate solutions) that are increasingly better suited to the
environment (problem). Commonly, the stopping criterion is either that
the maximum number of generation has been reached or a ``good enough''
solution has been found.

\subsection{Evaluation}
\label{EA.Evaluation}
Each individual is associated with a fitness value that measures how
well that individual solves the problem.  In many applications this is
formed by the application of some defined function to the candidate
solution. In others, the ``real world'' acts as the cost function. For
example, it is far easier to see how much power a program consumes by
running the program and measuring its power consumption than by
deriving and using a predictive model of power consumption (provided
one has the electronic skills to at such to carry out such
measurements). The fitness value assigned to each individual provides
a bias that is used to guide the search algorithm. Without this bias,
the search is no better than random search \cite{WB98}. From a
practical perspective, it is also essential to control the computation
complexity of the fitness evaluation.

\subsection{Selection}
\label{EA.Selection}
Individuals are selected based on their fitness value from the current
population to breed individuals of the next generations; fitter
individuals are selected preferentially over weaker ones. There are
many selection techniques, the most commonly used of which are:
\begin{description}
\item [Roulette wheel selection technique \cite{JHH75}] --- this
  technique is also known as fitness proportional selection. The
  probability of an individual being selected is proportional to its
  fitness values. This technique is simple but the selection pressure
  is highly sensitive to the scaling effect of the fitness
  values. Typically, the variance of fitness values in the population
  at the beginning of a run is very high; there is only a small number
  of individuals that are much fitter than others. This technique
  heavily selects these individuals. Consequently, the diversity of
  the population decreases at a very high rate, often resulting in
  premature convergence. Towards the end of a run, the variance of
  fitness values in the population becomes very small as the
  individuals are very similar to one another. Thus, the selection
  becomes similar to random selection.
\item [Rank based selection technique \cite{JEB85}] --- this technique
  uses fitness ranks instead of fitness values to determine the
  probability of an individual being selected. A typical
  implementation would assign probabilities according to an inverse
  linear relationship with rank. Thus, the probability of selecting
  the $k$-th fittest individual in a population of size $n$ is given
  by $(n+1-k)p$, where $k$ is the rank of the individual and
  $p=\frac{2}{n(n+1)}$. To determine the fitness rank however requires
  sorting on the fitness values, which can be expensive if the
  population size is large.
\item [Tournament selection technique \cite{AB81}] --- this technique
  selects individuals by holding multiple tournaments. In each
  tournament, $n$ individuals are selected from the population and the
  winner of the tournament is selected. $n$ is a tunable parameter
  known as the tournament size. The selection pressure increases with
  the tournament size. Binary tournament selection is the name given
  to this technique when $n = 2$.
\item [Elitist selection technique \cite{DT94, DT97}] --- the
  selection method used needs not to be probabilistic. Elitist
  selection is an example of deterministic selection in which $n$ of
  the fittest individuals in the current population are
  selected. Often, this technique is incorporated with other
  techniques so that a small portion of top performing individuals are
  guaranteed to be selected and copied to the population of the next
  generation. Other good performing individuals are selected
  probabilistically.
\end{description}

\subsection{Reproduction}
\label{EA.Reproduction}
Evolutionary operators are applied on these selected individuals
(commonly known as parents) probabilistically to produce individuals
of the next generation (commonly known children). Some common
evolutionary operators used are:
\begin{description}
\item [Crossover] --- parts of two selected individuals are exchanged
  to form two new individuals.
\item [Mutation] --- elements within the selected individual are
  perturbed in some way. This serves to diversify solution elements in
  the population. Given an initial population, repeated applications
  of selection and crossover alone might otherwise not be able to
  reach parts of the search space.
\item [Reproduction] --- the selected individual is passed on to the
  next generation unchanged. This serves to preserve the best
  performing individuals (since specific instances of applying
  evolutionary operators do not reliably produce better individuals).
\end{description}

\subsection{Implementations of EAs}
\label{EA.TypesOfEA}
Traditionally, EAs can be divided into the following four categories
based on the individual representation adopted \cite{AEE03}:
\begin{enumerate}
\item Genetic Algorithms that use fixed length binary string
  individuals.
 \item Genetic Programming that uses tree structure individuals.
 \item Evolution Strategies that use real valued vector individuals.
 \item Evolutionary Programming that uses finite state machine
   individuals.
\end{enumerate}
However, the boundaries between these categories become increasingly
blurred. Often the properties from different categories are combined
to suit the problem in question. For example, Binary Genetic
Programming (BGP) \cite{WB94} uses a binary string individuals that
encode tree structures, Cartesian Genetic Programming (CGP)
\cite{JFM00} uses binary string individuals that encode graphs, etc.

\section{Genetic Programming (GP)}
\label{GP}
Genetic Programming (GP) is a form of EAs in which an individual is
typically a program represented by a tree structure.  An example that
implements the formula $(X \times Y) + (4 - (Y + 1))$ is shown in
Figure \ref{fig:GPTree}.

\begin{figure}[htbp]
  \centering
  \resizebox{!}{10mm}{
    \begin{bundle}{\gpbox{$+$}}\chunk{\begin{bundle}{\gpbox{$\times$}}\chunk{\gpbox{X}}\chunk{\gpbox{Y}}\end{bundle}}\chunk{\begin{bundle}{\gpbox{$-$}}\chunk{\gpbox{4}}\chunk{\begin{bundle}{\gpbox{+}}\chunk{\gpbox{Y}}\chunk{\gpbox{1}}\end{bundle}}\end{bundle}}\end{bundle}
  }
\caption{A GP individual that represents a formula $(X \times Y) + (4
  - (Y + 1))$.}
\label{fig:GPTree}
\end{figure}

The tree nodes can be classified into two groups: the terminal set $T$
and the function set $F$. The terminal set $T$ consists of constants
and variables (the leaf nodes) and the function set $F$ consists of
functions, operators and statements (the non-leaf nodes). An example
of terminal set $T$ and function set $F$ which is sufficient to allow
a description of the tree in Figure \ref{fig:GPTree} is:
\begin{align*}
  T &= \{ X, Y \} \bigcup \{1, 2 \ldots 10\} \\
  F &= \left\{ +, -, \times, \div, \min, \max \right \}
\end{align*}
These sets must fulfil the sufficiency and closure properties.  The
sufficiency property requires that the target solution for the problem
in question can be represented with the elements in the sets. The
closure property requires that the elements in function set $F$
can accept any value they may receive as input, including all the
elements in terminal set $T$ and any return values from other
functions or operators.  Practically, it is important to keep both
sets small to prevent the search space from becoming too large.

In GP, the crossover operator is performed on two trees.  A subtree
in each tree is chosen randomly and swapped with the other. An example
of crossover operation is depicted in Figure
\ref{fig:CrossoverAndMutation}, where the marked subtrees in the two
trees are swapped with each other, resulting in two new trees. The
mutation operator is performed on one tree. A node in a tree is chosen
randomly and replaced with a randomly generated new node or subtree
randomly. An example of mutation operation is depicted in Figure
\ref{fig:CrossoverAndMutation}, where the marked node is replaced with
a new subtree. Other operators also exists, e.g., reproduce operator
copies the selected individual to the population of the next
generation without changes, permutation operator changes the node
values of a subtree randomly, etc.

\begin{figure}[htbp]
  \centering
  \subfigure{
    \resizebox{!}{10mm}{
        \begin{bundle}{\gpbox{$+$}}
        \chunk{
                \begin{bundle}{\gpboxhighlight{$+$}}
                \chunk{\gpbox{X}}
                \chunk{
                        \begin{bundle}{\gpbox{$+$}}
                        \chunk{\gpbox{$Y$}}
                        \chunk{\gpbox{$3$}}
                        \end{bundle}
                        }
                \end{bundle}
        }
        \chunk{
                \begin{bundle}{\gpbox{$-$}}
                \chunk{\gpbox{$4$}}
                \chunk{\gpbox{Y}}
                \end{bundle}
        }
        \end{bundle}
        $+$
        \begin{bundle}{\gpbox{$-$}}
        \chunk{\gpbox{$3$}}
        \chunk{\begin{bundle}{\gpboxhighlight{$\times$}}
                \chunk{\gpbox{X}}
                \chunk{\gpbox{Y}}
                \end{bundle}
        }
        \end{bundle}
        $\xrightarrow{\text{crossover}}$
        \begin{bundle}{\gpbox{$+$}}
        \chunk{
                \begin{bundle}{\gpboxhighlight{$\times$}}
                \chunk{\gpbox{X}}
                \chunk{\gpbox{Y}}
                \end{bundle}
        }
        \chunk{
                \begin{bundle}{\gpbox{$-$}}
                \chunk{\gpbox{$4$}}
                \chunk{\gpbox{Y}}
                \end{bundle}
        }
        \end{bundle}
        $+$
        \begin{bundle}{\gpbox{$-$}}
        \chunk{\gpbox{$3$}}
        \chunk{
                \begin{bundle}{\gpboxhighlight{$+$}}
                \chunk{\gpbox{X}}
                \chunk{
                        \begin{bundle}{\gpbox{$+$}}
                        \chunk{\gpbox{$Y$}}
                        \chunk{\gpbox{$3$}
                        }
                        \end{bundle}
                }
                \end{bundle}
        }
        \end{bundle}
      }
    }
  \subfigure{
    \resizebox{!}{10mm}{
    \begin{bundle}{\gpbox{$+$}}\chunk{\begin{bundle}{\gpboxhighlight{$+$}}\chunk{\gpbox{X}}\chunk{\begin{bundle}{\gpbox{$+$}}\chunk{\gpbox{$Y$}}\chunk{\gpbox{$3$}}\end{bundle}}\end{bundle}}\chunk{\begin{bundle}{\gpbox{$-$}}\chunk{\gpbox{$4$}}\chunk{\gpbox{Y}}\end{bundle}}\end{bundle}
    $\xrightarrow{\text{mutation}}$
    \begin{bundle}{\gpbox{$+$}}\chunk{\begin{bundle}{\gpboxhighlight{$\times$}}\chunk{\begin{bundle}{\gpbox{$\times$}}\chunk{\gpbox{$Y$}}\chunk{\gpbox{$Y$}}\end{bundle}}\chunk{\begin{bundle}{\gpbox{$+$}}\chunk{\gpbox{$2$}}\chunk{\gpbox{$4$}}\end{bundle}}\end{bundle}}\chunk{\begin{bundle}{\gpbox{$-$}}\chunk{\gpbox{$4$}}\chunk{\gpbox{Y}}\end{bundle}}\end{bundle}
    }
  }
  \caption{The crossover and mutation operations in Genetic Programming.}
  \label{fig:CrossoverAndMutation}
\end{figure}

\subsection{Extensions on GP}
\label{GP.ExtensionsonGP}
In the standard form of GP, there is no way to ensure that all the
functions in each tree have appropriate data types
(children). Strongly Typed Genetic Programming (STGP) is introduced in
\cite{DJM95}. STGP augments each node with a type. A set of type rules
is defined to specify how nodes in a tree can be connected with one
another. For example, we may require that $\geq$ take two floating
point children and return a boolean value. The population
initialisation and evolutionary operations must obey the type
rules. In STGP, only well-typed individuals are maintained in the
population.

Another way to ensure type conformance is to use grammar to control
the individual tree structure. However, the run-time grammar
conformance checking is very expensive. In Section \ref{GE}, an
alternative EA --- Grammatical Evolution (GE) that is able to evolve
individuals in a grammar compliant way is presented.

Automated Defined Functions (ADFs) \cite{JRK92} are ``building
blocks'' observed during the evolution process. They can be recognised
and subsequently be made available as units in the evolutionary
process, effectively implementing an on-the-fly modularisation
process.

Alternative individual representations are also being proposed. For
example, Cartesian Genetic Programming (CGP) \cite{JFM00} uses integer
strings as individuals with each encoding a graph representation of a
program. Constraints are defined on how the integer strings can evolve
in order to ensure the validity of the artifact as represented by the
indexed graph. This concept is very similar to the use of integer
strings, which serve as indexes to select which grammar rules are to
be expanded in Grammatical Evolution (GE) (cf. Section
\ref{GE}). Linear Genetic Programming \cite{WB98, MB01} uses
individuals, which have variable length sequences of simple C
instructions that operates on one or two indexed variables (registers)
or constants from a predefined set. Alternatively, PushGP \cite{LS02}
is used to evolve stack based execution instructions.

%\subsection{Review}
%\label{GP.Review}
%GP has been used to solve problems in many different domains,
%including the placement and routing in electrical circuit, the size
%and structure of antenna, controller, genetic networks, cellular
%automata, data mining and rule induction, etc. At present, there are
%$36$ known human competitive results achieved, in which $21$ of the
%results infringe on existing patents and $2$ instances resulted in new
%patentable inventions \cite{JRK03}. DA DA DA

\section{Grammatical Evolution (GE)}
\label{GE}
Grammatical Evolution (GE) uses variable length binary string
individuals which serve as indices that map a set of BNF language
grammar rules to programs \cite{CR98}. These programs can be executed
for fitness evaluation purposes. To solve a problem using GE, the
language grammar must first be specified in Backus Naur Form
(BNF). The grammar of a language in BNF is represented as a tuple
$\{N$, $T$, $S$, $P\}$ where $N$ is the non-terminal set, $T$ is the
terminal set, $S$ is a special element of $N$ called start symbol
which is first expanded. $P$ is the set of production rules (also
called derivation rules) that is used to guide the expansion from $N$
to $T$. A production rule has the following format:
\begin{alltt}
<non-terminal> ::= <expr-of-symbols-1>
                |  <expr-of-symbols-2>
                |  ...
\end{alltt}

The production rule states how the non-terminal symbol on the left
hand side of the rule is to be substituted by one of the symbol
expressions on the right hand side of the rule. $|$ is used to
indicate the choice of expressions. An example of BNF grammar is as
follows:
\begin{align*}
  N &= \{\texttt{<expr>},~\texttt{<op>},~\texttt{<var>}\}\\
  T &= \{\texttt{+},\texttt{-},\texttt{$\times$},\texttt{$\div$},\texttt{X},\texttt{Y},\texttt{(},\texttt{)}\}\\
  S &= \texttt{<expr>}
\end{align*}
and $P$ consists of a set of production rules as follows:
\begin{alltt}
<expr> ::= <expr> <op> <expr>           (0)
        |  (<expr> <op> <expr>)         (1)
        |  <var>                        (2)

<op>   ::= \(+\)                           (0)
        |  \(-\)                           (1)
        |  \(\times\)                           (2)
        |  \(\div\)                           (3)

<var>  ::= X                            (0)
        |  Y                            (1)
\end{alltt}
This grammar defines a set of arithmetic expressions over the
variables \texttt{X} and \texttt{Y}. Although there is no distinction
made on the BNF grammar between the functions and terminals in GE,
this distinction is still required at the implementation level.

\subsection{Genotype-phenotype Mapping}
\label{GE.GenotypePhenotypeMapping}
The mapping process between the variable length binary string
individuals and programs using BNF grammar is known as
genotype-phenotype mapping\footnote{In the biological world, a
  genotype is the internally coded information (genes) that is
  inherent in an individual. The phenotype is the ``observable
  characteristics'' of an individual that can be influenced by the
  genotype and environment. For example, the genotype of human being
  is the DNA and an example of the phenotype is his eye colour.}. This
is best illustrated with an example. In each individual (genome),
every $8$ consecutive bits is viewed as an integer which are more
commonly known as a a codon. Consider an individual with the following
codons:
\begin{equation*}
  [10,~5,~51,~8,~16,~49,~30,~18] 
\end{equation*}

As mentioned, the mapping process begins with the start symbol. The
codon value is read to determine the usage of a certain production
rule to expand the symbol using a modulus mapping function as follows:
\begin{equation*}
  \textit{rule} = \textit{codon value } \% \textit{ total number of applicable production rules} 
\end{equation*}
where $\%$ is the arithmetic modulus operator. Assuming the start
symbol is \verb|<expr>|, the first codon value is $10$ and there are
$3$ immediately applicable production rules for \verb|<expr>|,
therefore $10$ $\%$ $3 = 1$. The production rule $1$ is used to
replace \verb|<expr>| with \verb|(<expr> <op> <expr>)|. Assuming the
leftmost derivation is used, the leftmost \verb|<expr>| is first
expanded as \verb|(| is a terminal. The second codon value is used to
select the production rules. As $5$ $\%$ $3 = 2$, the production rule
$2$ is selected and the leftmost \verb|<expr>| is replaced by
\verb|<var>| resulting in \verb|(<var> <op> <expr>)|. The next
non-terminal to be expanded is \verb|<var>|. As the next codon is $51$
and the number of rule choices now available is $2$, \verb|<var>| is
replaced with \verb|Y|.

The process continues until a complete program is generated, i.e.,
when all the non-terminals have been transformed into terminals. While
this is desirable, some problems can arise during the mapping
process. Firstly, there is a possibility that the codon values may run
out before the entire mapping process is complete. To overcome this
problem, individual can be wrapped around to reuse the codon value,
i.e., the reading of the last codon ($18$ in the example) will be
followed by the first codon ($10$ in the example).

The mapping process also can be indefinitely long if the context free
grammar used is recursive. Consider the production rules for
\verb|<expr>|. If the codon value always maps onto the production rule
$0$ or $1$, the mapping process is never ending. The \verb|<expr>| is
replaced by either \verb|<expr> <op> <expr>| or
\verb|(<expr> <op> <expr>)|. In both cases, the next leftmost
non-terminal is again \verb|<expr>| itself. This problem can be
resolved by setting an upper bound on the number of times the
production rules can be performed. In the original algorithm,
this is achieved indirectly by setting a limit on the number of times
the individual can be wrapped around and thus the number of codons
available. If a complete program is not fully generated (the
non-terminals have not been transformed to terminals) when the wrap
around threshold is reached, this individual is given the lowest
possible fitness value to decrease the probability of this individual
to be selected in selection step.

\subsection{Extensions on GE}
\label{GE.ExtensionsonGE}
The greatest advantage of GE is that it allows artifacts to be evolved
in a grammar compliant way and many solution spaces can be defined
using grammar. Mechanisms for explicit prevention or repair of invalid
artifacts are therefore not required.

The use of grammar also makes the languages and search algorithms
become independent components. The program generation in arbitrary
languages is possible by only changing the BNF grammar. The search
algorithm can be replaced with other algorithms. Instead of using
Genetic Algorithms, it is possible to use other search techniques. For
example, Grammatical Swarm \cite{MON04} uses Particle Swarm
Optimisation (PSO) to carry out the search. This feature allows GE to
reap the advantages of any improvement in any evolutionary algorithm.

Although the lengths of individuals can be set within a pre-defined
range, this does not mean that the sizes of the generated programs
will be similar. In many problems, it has been found that many of the
randomly generated individuals fail to complete the genotype-phenotype
mapping in the initial population generation, even when the
individuals are allowed to wrap around themselves up to $30$ times
\cite{CR03}. To overcome this problem, a sensible method \cite{CR03}
is proposed to take the grammar into account in the initialisation
process. It is modelled after the popular \emph{ramp-half-and-half}
initialisation method \cite{JRK92}, which takes the population size,
minimum and maximum heights of the trees permitted in the population
as input parameters and generates approximately $50\%$ full trees with
maximum height while the other $50\%$ have heights between the minimum
and the maximum. This is achieved by first assigning each rule in the
grammar with two properties: the minimum number of mapping required
for the non-terminal on the left hand side of the rule to be
completely mapped to terminals and whether the rule is recursive. As
each derivative tree is built, only rules that can fit the remaining
depth of the tree are selected. To generate full trees, the recursive
rules are always chosen whenever possible. To generate trees with
variable heights between the minimum and maximum limits, the recursive
and non-recursive rules are chosen with equal probabilities. The final
step in the initialisation is to reversely map the nodes of the tree
to the corresponding codons.

Additionally, the traditional one point crossover, which randomly
chooses a point on each of the two selected individuals and swaps all
data beyond that point, may not be very effective in GE. This is
because the chosen crossover point may be located at a position after
the effective length of the individual (the portion of the individual
that is actually used to select the rules) and render the crossover
operation ineffective. Effective crossover restricts the chosen
crossover point in the range of effective length.

%%%% DA DA DA
%However, the use of grammar based mapping also introduces its own problem. other limitation. The problem dependencies discused earlier \ref{X} is exarberated in GE.
%
%Much ripple crossoever. 

% The use of modulus operator also introduces bias in the selection of
% the rules. For example there is only two production rules is available
% an expnasion of a variable, the only bit that matter is the least
% singnificant bit; other bits do not relaly matter at all. An
% improvement to this is proposed in in which.

%%%%%
\section{Multi-objective Evolutionary Algorithms (MOEAs)}
\label{MOEA}
In many practical problems, a desire to optimise more than one
objective is common, some of which can be in conflict with one
another. Instead of having a single fitness score, each possible
solution has a vector of fitness scores, one per objective. This is
typically referred to as a multi-objective optimisation (MOO)
problem. To visualise this, the concepts of the solution space $S$ and
objective space $O$ are used. An example of $S$ and $O$ for a
minimisation with two objectives is shown in Figure
\ref{fig:mapping}. Each point $s \in S$ represents a possible solution
to the problem in question. The fitness function, $f$ maps a point $s
\in S$ to a point $o \in O$ such that the location of the point $o$
represents how well $s$ meets the objectives. The fitness function $f$
is surjective (onto) but not injective ($1$ to $1$); each point in $S$
is mapped to a point in $O$ and multiple points in $S$ can be mapped
to one point in $O$. The region in which all the implementable
solutions reside within is known as the feasible solution region.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{mapping}
\caption{The mapping between solution space, $S$ to objective space, $O$.}
\label{fig:mapping}
\end{figure}

The aim of multi-objective evolutionary algorithms (MOEAs) is to
discover the set of solutions with the optimal tradeoffs between all
the fitness functions. Referring to the above example, these solutions
are on the dotted lines in the objective space in Figure
\ref{fig:mapping}.

\subsection{Weighted Sum of Fitness Functions}
% The traditional way to solve MOO problems in order to aggregate the
% vector of fitness scores using a weighted sum fitness function is as
% follows:
One widely used approach to solve MOO problems is to use a weighted
sum of the individual fitness functions as the overall fitness function
for the problem, as indicated below:
\begin{equation}
  f(x)~=~\sum^n_{m=1}w_{m}f_{m}(x)
\label{eq:WSFF}
\end{equation}
where $w_m$ is the a scalar weight for $f_{m}$. However, this approach
requires the relative weights of all the objectives to be
pre-determined. This is often difficult. For example, how many times
higher/lower is the risk of an unauthorised disclosure of one
information element in comparison to the others?  This is like
``comparing apples with oranges''; there is inevitably some degree of
subjectiveness and arbitrariness in weight assignments. A more
principled and conceptually clear approach would be advantageous.

%Defining a set of relative weights essentially defines a \emph{single}
%optimum target solution. Evolutionary runs are used to search and
%converge the population to this single target. 

Each chosen set of weights essentially defines a problem landscape via
the single fitness function defined in \eqref{eq:WSFF}. The space can
then be searched using any appropriate technique. If the tradeoffs
among objectives change, the technique has to be rerun to search for
the new optimum solution. This can be very inefficient. Moreover, for
a certain set of problems, this approach is unable to discover all
solutions with different levels of tradeoffs. Consider a two objective
minimisation problem with its objective space shown in Figure
\ref{fig:concavefeasibleregion}, the set of solutions that lie at the
concave part of the feasible region can never be discovered. This is
because this approach only searches a single optimal point, which lies
at the intersection between a tangent to the feasible region, where
the gradient of the tangent is defined by the relative weights of the
objectives used.

% The set of weights of the fitness functions used essentially sets the
% location of the target optimum solution to be at the intersection
% between a tangent, which has its gradient defined by the relative
% weights of the fitness functions, to the feasible region.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{concave}
\caption{The set of solutions that lie at the concave part of the
  feasible region can never be discovered.}
\label{fig:concavefeasibleregion}
\end{figure}

\subsection{Pareto Front based Approaches}
Pareto front based approaches search for multiple optimal solutions in
one single evolutionary run using the Pareto dominance concept in the
fitness evaluation of each solution (individual).  A Pareto dominance,
$\succ$ relation between two solutions, $x$ and $y$ is defined as
follows:
\newtheorem*{theorem2}{Pareto dominance, $\succ$}
\begin{theorem2}
  Let $f_{x}$ and $f_{y}$ be the fitness vectors of $x$ and
  $y$, $>$ denote ``is fitter than''
  relation, $x \succ y \Leftrightarrow \forall i \cdot f_{x}[i] \geq
  f_{y}[i] \wedge \exists i \cdot f_{x}[i] > f_{y}[i]$ where $f[i]$ is
  the fitness score of the $i$-th objective.
\end{theorem2}

In other words, $x$ dominates $y$ iff $x$ is better than $y$ in at
least one aspect and is at least equally good in all other
aspects. This defines a partial order relation on the solution
space. There always exists a subset of solutions that are not
dominated by any other solutions. This subset represents the best
solutions possible and is known as the \emph{Pareto front} or the
\emph{Pareto optimal set}.

The aim of Pareto front based approaches is to converge the
individuals in the population to the Pareto optimal set of
solutions. Within the population of each generation, there is always a
subset in which solutions are not dominated by any other solution in
the population. The aim is to make this non-dominated subset a better
approximation of the Pareto optimal set than the one in the previous
generation. To have a good approximation to the Pareto optimal set,
these approaches also attempt to maximise the diversity among the
solutions in the non-dominated subset.  Consider an optimisation
problem with two objectives such as in Figure \ref{fig:MOEA}, let the
curve line represent all the solutions with the best possible
achievable tradeoffs between the two objectives, i.e., the real Pareto
front, MOEA attempts to converge the individuals (points) in the
population to be as near to the Pareto front as possible and also as
diversely spread on the Pareto front as possible.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{MOEA}
  \caption{The ultimate aim of the Pareto front based approaches is to
    obtain the best approximation of the Pareto front of the problem
    in question with the individuals in the population.}
  \label{fig:MOEA}
\end{figure}

\subsection{Advantages of Pareto Front based Approaches}
The Pareto front based approaches have several advantages over the
traditional weighted sum of fitness functions approach:
\begin{itemize}
\item The weights that define the tradeoffs among different objectives
  are no longer required to be determined a priori. Such a
  determination is difficult as it requires a deep understanding of
  the problem domain.
\item The Pareto front can reveal the relationship between different
  objectives, which may be difficult to obtain otherwise. Such
  information is helpful in guiding a decision maker to choose the
  optimal solution for the problem from the Pareto optimal set.
\item The set of Pareto optimal solutions can be saved and be
  retrieved later. Such a retrieval may be necessary if a change in
  circumstance requires a different set of tradeoffs and therefore a
  different solution.
\item Optimising for multiple fitness scores tends to preserve the
  diversity of the population, which prevents the population from
  being trapped in a local optimum and increases the chance of finding
  better solutions.
\end{itemize}

\subsection{Implementations of Pareto Front based Approaches}
There are many different implementations. Two of the most popular
implementations: SPEA2 \cite{EZ01} and NSGA2 \cite{KD00} are presented
here. As EAs are stochastic in nature, there is no guarantee that the
population will converge to the real Pareto front, i.e., the output is
only an approximation of the real Pareto front.

\subsubsection{Strength Pareto Evolutionary Algorithm 2 (SPEA2)}
As suggested by its name, SPEA2 is an improvement over its previous
implementation, SPEA \cite{EZ98}. SPEA2 is an elitist approach which
uses a fixed size archive that stores all non-dominated individuals
found in each generation. Binary tournament selection is then used to
select individuals to reproduce offspring population using
evolutionary operators. The best $n$ individuals from the union of the
archive and offspring population formed become the population of the
next generation, where $n$ is the population size.

As the archive size is fixed, three possible cases can arise: the
number of non-dominated individuals is equal to, lesser than or more
than the archive size. In the first case, there is no additional work
required.

If the number of non-dominated solutions is less than the archive
size, the remaining vacancies are filled with non-dominated
solutions. The selection is based on the fitness values of the
individuals. The fitness value of an individual is determined by two
factors:
\begin{description}
\item [Raw fitness] --- the sum of the strength of individuals it is
  dominated by. The strength of an individual is the number of
  individuals it dominates.
\item [Density estimation] --- the inverse of the Euclidean distance
  of the individual to the $k$-$th$ nearest neighbour in the objective
  space, where $k$ is usually set to be the square root of the sum of
  population size and archive size.
\end{description}
In SPEA2, a smaller fitness value means fitter and vice versa.

If the number of non-dominated solutions is more than the archive
size, the non-dominated solution that has the shortest Euclidean
distance to another individual in the objective space is dropped. If
two solutions have the same distance to their nearest neighbours, the
tie is broken by comparing their distances to their second nearest
neighbours and so forth. This process is iterated until the
non-dominated solutions can fit into the archive. Essentially, the
goal is to fill the archive with non-dominated solutions as uniformly
and widely distributed over the objective space as possible. Referring
to Figure \ref{fig:DOO}, an example of the formed Pareto front and the
solutions that are to be kept in the archive is depicted.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\textwidth]{DOO}
\caption{The SPEA2 truncates its archive such that individuals that
  are close together are removed \emph{iteratively} until they can fit
  into the archive. Assuming the archive size is $10$, the individuals
  removed are shown.}
\label{fig:DOO}
\end{figure}

\subsubsection{Non-dominated Sorting Genetic Algorithm 2 (NSGA2)}
Similarly to SPEA2, NSGA2 is an improvement algorithm over its
previous implementation NSGA \cite{NS94}. NSGA2 is also an elitist
algorithm that uses an archive to preserve the non-dominated
solutions. The main distinction between these two algorithms is in the
way they preserve the elites.

In each generation, NSGA2 uses the Pareto dominance relation to divide
the population into partitions with different rank levels. The
non-dominated solutions in the population are first assigned to
partition rank $1$; the non-dominated solutions in the remaining
population are assigned to partition rank $2$, and so forth. This
process is continued until all solutions are assigned to a
partition. The rank of the partition is then used as the order to
admit solutions into the archive; partition with better (lower) rank
are admitted first. Solutions are admitted on partition by partition
basis until the archive is filled or has exceeded its maximum
capacity.

If the archive size has exceeded its maximum capacity, some of the
solutions from the last admitted partition are removed based on the
crowding distance. The crowding distance of a solution is the average
distance of two adjacent solutions on either side of the solution
along each of the objectives in the objective space. The individual
with the smallest crowding distance is removed iteratively until the
solutions can fit the archive.

Individuals are then selected from the archive using crowded binary
tournament to reproduce an offspring population of the same size using
evolutionary operators. The crowded binary tournament selects the
better of two randomly selected individuals, where better means the
lower rank individual or the one with lower density measure (higher
crowding distance) in cases where both of them have equal rank. The
union of the archive and the offspring population forms the population
of the next generation.

\section{Applications of GP and GE in Related Domains}
\label{EA.PreviousApplicationsOfGPandGE}
This section reviews some of the applications of GP and GE in the
related domains including rules inference, intrusion and anomaly
detection systems, security protocols and cryptography.

\subsection{Rules Inference}
\label{EA.Application.RulesInference}
A grammar based GP system called LOGENPRO (LOGic grammar based GENetic
PROgramming system) is proposed in \cite{MLW00}. In the experiment,
the result shows that LOGENPRO is able to outperform some Induction
Logic Programming (ILP) systems in inferring rule sets. In
\cite{RRFM01} a GP experiment on coevolution between rules and fuzzy
membership variables is designed. The experimental result shows that
the output set of rules and variables are well adapted to one
another. In \cite{GLP04} an attempt is made to invent a generic rule
induction algorithm using grammar based GP. The result is shown to be
competitive with some well-known manually designed rule induction
algorithms.

GE is used to learn investment strategy (trading rules) using the
market stock indices in \cite{TB01, MON01, ID06}. The results show the
investment strategy inferred yields profitable performance for the
trading periods analysed. In \cite{TB02, AB04} GE is used to evolve
rules for foreign exchanges. The rules learnt outperform the
benchmark buy and hold strategy over all the trading periods analysed.

\subsection{Intrusion and Anomaly Detection Systems}
\label{EA.Applications.IntrusionAndAnomalyDetection}
EAs have also been used to discover useful patterns of system features
that describe the system behaviour. These discovered features are then
used to recognise anomalies and intrusions.

In \cite{AA06}, GP is used to evolve an intrusion detection
program. In comparison with the results obtained using other machine
learning approaches (support vector machine (SVM) and decision tree),
GP is found to be able to generate a program that is more compact,
efficient and selective in choosing the relevant input features. In
\cite{MC95}, GP is shown to be able to evolve a set of autonomous
agents that are used to implement an intrusion detection system. In
\cite{WD06}, GE is also shown to be effective in generating an
intrusion detection program.

Evolving an intrusion detection system or a computer program for
MANETs using EAs is a relatively new domain. Often, MOEAs are used to
search for programs that are not only optimal in accomplishing
functional objectives, but are also optimal against other
non-functional objectives, e.g., the program size, memory requirement,
power consumption, etc. These objectives are important considering
that the operational environment has very limited resources available.

In \cite{TW06}, a MOEA based framework that is able to evolve
efficient distributed programs for sensor networks is presented. The
election problem is used to demonstrate the feasibility of the
framework. In \cite{SS08}, GE is used to evolve an intrusion detection
system for MANETs. The result shows that the system is promising. %A
%similar attempt is made to evolve the power-aware intrusion detection
%system for the same network using GP in \cite{X}.

\subsection{Security Protocols}
Genetic Algorithms are used to synthesise abstract security protocols
in a small subset of BAN logic automatically \cite{JAC00}.  The
synthesised abstract security protocols are provably correct with
respect to the set of logic used. These works are extended to cover
more complicated protocols in BAN logic \cite{HC03} as well as more
sophisticated SVO logic \cite{HC04}.

Although Genetic Algorithms are claimed to be the search algorithm
used, the use of binary string individuals which are treated as
integer strings, the use of modular operator to reduce these integers
to indices within bounded ranges, and the use of these indices to
select which element it represents in each protocol component are very
similar to GE.

\subsection{Cryptography}
In \cite{JCHC06}, GP is applied to evolve block cipher which plays a
significant role in encryption. In \cite{JP08}, an extremely
lightweight and fast block cipher is successfully evolved using
GP. This block cipher is competitive with the well respected Tiny
Encryption Algorithm (TEA) \cite{DJW94} in terms of security strength
and speed.

In \cite{MR08}, GE is shown to be able to evolve some of these boolean
functions with cryptographic significance. The design of boolean
functions that is suitable for cryptographic schemes is difficult
because the desired properties of the functions are complicated and
often conflicting with one another.

In \cite{PM06}, GP is shown to be able to evolve Quantum Fourier
Transform which is an important building block in the Shor's algorithm
\cite{PWS94, PWC97}. The discovery of this algorithm is significant
as it is able to solve factorisation and discrete logarithms in
polynomial time. Many widely respected encryption algorithms rely on
these problems being computationally intractable.

\section{Fuzzy Expert Systems}
\label{Fuzzyexpertsystems}
Unlike in the computer world where all the operations are essentially
binary, our world is full of ambiguities. The concept of fuzzy logic
was conceived by Zadeh as a way to deal with this ambiguity
\cite{LAZ65}. Instead of determining a statement as being either true
or false, a degree of truth is associated with the statement using a
real value in the range $[0.0, 1.0]$ where $0.0$ means absolute false
and $1.0$ means absolute truth.

\subsection{Fuzzy Set Theory}
In classical set theory, an element $x$ can either be a member of a
set, $S$ or the complement of the set $S'$, but not both. Fuzzy set
theory extends the classical set theory to allow partial membership. A
value in the range $[0.0, 1.0]$ is introduced to describe the
membership of $x$, with $0.0$ representing complete non-membership,
$1.0$ representing absolute complete membership and values in between
representing the degree of partial membership \cite{MK93}. Often, this
membership assignment is achieved using a function called membership
function.

$3$ basic fuzzy set operators, namely \emph{union} ($\cup$),
\emph{intersection} ($\cap$) and \emph{complement} ($'$) are defined in
analogous to the classic set operators. Let $\mu_A(x)$ and $\mu_B(x)$
be the fuzzy membership functions of the fuzzy set $A$ and $B$, these
operators are defined as follows \cite{LAZ65}:
\begin{align}
  \mu_{A \cup B}(x) &= \max(\mu_A(x),~\mu_B(x)) \label{eq:max}\\
  \mu_{A \cap B}(x) &= \min(\mu_A(x),~\mu_B(x)) \label{eq:min}\\
  \mu_{A'}(x) &= 1 - \mu_A(x) \label{eq:complement}
\end{align}

There is a shortcoming with this definition: the result of the
operations depends only on one of the potentially many operands
\cite{DIB95}. There are other definitions proposed for these
operators. Many of these proposals attempt to redefine the
\emph{union} and \emph{intersection} operators, but leave the
\emph{complement} operator unchanged.

In the same paper, two additional operators, namely \emph{algebraic
  sum} ($\oplus$) and \emph{algebraic product} ($\otimes$), are
introduced and defined as:
\begin{align}
  \mu_{A \oplus B}(x)  &= \mu_A(x) + \mu_B(x) - \mu_A(x)\mu_B(x) \label{eq:algebraicsum} \\
  \mu_{A \otimes B}(x) &= \mu_A(x)\mu_B(x) \label{eq:algebraicproduct}
\end{align}
As the $\oplus$ and $\otimes$ are equivalent operators to $\cup$ and
$\cap$ in classical set theory respectively, these operators are
sometimes viewed as alternative definitions for $\cup$ and $\cap$
operators.  Indeed, these definitions are used in the Fuzzy MLS model
introduced in Section \ref{FuzzyMLSModel}. These operators are
different in fuzzy set theory as $\mu(x)$ is no longer a binary value.

Yager proposed an alternative set of interpretations for $\cup$ and
$\cap$ operators in \cite{RRY80} as follows:
\begin{align}
  \mu_{A \cup B}(x) &= \min[1,~({\mu_A(x)}^w + {\mu_B(x)}^w)^{\frac{1}{w}}] \\
  \mu_{A \cap B}(x) &= 1 - \min[1,~((1 - {\mu_A(x)})^w + (1 - {\mu_B(x)})^w)^{\frac{1}{w}}]
\end{align}
with $w \geq 1$. For $w = 1$, Yager's operators are reduced to Zadeh's
\emph{algebraic sum} \eqref{eq:algebraicsum} and \emph{algebraic
  product} \eqref{eq:algebraicproduct} operators. For $w = \infty$,
Yager's operators are reduced to Zadeh's \emph{union} \eqref{eq:max}
and \emph{intersection} \eqref{eq:min} operators. The comparison
between the behaviours of these two definitions is shown in the Figure
\ref{fig:fuzzyoperationscomparison} \cite{DIB95}. Refer \cite{DD85,
  HZ01} for other interpretations.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{FuzzyOperationsComparison}
  \caption{A comparison between different interpretations of the fuzzy
    operations.}
  \label{fig:fuzzyoperationscomparison}
\end{figure}

In all these interpretations, the behaviours of the operators are the
same as in the classical set theory if the membership value is
restricted to only binary value ($0$ or $1$). Therefore, fuzzy set
theory and logic can be thought of as a generalisation of classical
set theory and logic. Many of the interpretations proposed attempt to
preserve the laws defined in classical set theory, e.g., Commutative
Law, Distributivity, De Morgan's law, etc. However, the Law of
Contradiction $A \cup A' \equiv U$ (a set and its complement must
establish the universe of disclosure) and Law of Excluded Middle, $A
\cap A' \equiv \emptyset$ (an element can either be in a set or not,
but not in both) are not established in fuzzy set theory. Indeed,
fuzzy set theory can exist only if these two laws do not hold.

Lastly, the fuzzy set theory ought not to be confused with the
probability theory. While they are similar mainly due to the use of
value in the range $[0.0, 1.0]$, there is no restriction on fuzzy set
theory to enforce the concept that the sum of all memberships has to
be equal to $1.0$, as in probability theory.

\subsection{Inference Process}
One major application of fuzzy logic is to build fuzzy expert
systems. A fuzzy expert system consists of a set of fuzzy membership
functions and inference rules that are used to reason about data
\cite{MK93}. These systems have achieved many successes in different
fields, including controller, pattern recognition and data
analysis. The inference rules take the following form:
\begin{alltt}
  IF x is low and y is medium THEN z is high
\end{alltt}
where \texttt{x}, \texttt{y} are the input variables and \texttt{z} is
the output variable. The \texttt{low}, \texttt{medium} and
\texttt{high} are the membership functions defined on \texttt{x},
\texttt{y} and \texttt{z} respectively. The antecedent (the part
between \texttt{IF} and \texttt{THEN}) is the condition that describes
the degree of truth and the degree of application of this rule. The
conclusion (the part after \texttt{THEN}) assigns membership functions
to the output variables.

Applying these inference rules to the data, we can infer the values of
output variables for any given specific values of input
variables. This inference process consists of the following steps
\cite{MK93, FLT07}:
\begin{description}
\item [Fuzzification] --- The input values are mapped to fuzzy
  membership values using the membership functions. In each rule, if
  the antecedent consists of more than one input variable, the input
  fuzzy membership values are aggregated using fuzzy set
  operators. The interpretation of these operators is a design issue
  of the system.
\item [Inference] --- These aggregated fuzzy membership values are
  used to determine the degree of application of each rule. The fuzzy
  membership values are applied to each output variable for each
  rule. There are two common inference methods which are \emph{min}
  and \emph{product} inferencing. These are essentially equivalent to
  the two Zadeh's \emph{intersection} and \emph{algebraic product}
  fuzzy operators respectively.
\item [Composition] --- All the fuzzy membership values assigned to
  each output variable in each rule are combined to form a single
  fuzzy membership value. There are two common composition methods,
  depending on which inferencing method has been used in the previous
  step. If the \emph{min} inferencing is used, the \emph{max}
  composition method, which is essentially equivalent to Zadeh's
  \emph{union} operator, is typically used. If the \emph{product}
  inferencing is used, \emph{sum} composition method, which is
  essentially equivalent to Zadeh's \emph{algebraic sum} operator, is
  typically used. An alternative interpretation of \emph{sum}
  composition method is the sum of fuzzy membership values of all the
  output variables in each rule. This interpretation has the advantage
  of a simpler computation at the price of a lower accuracy.
\item [Defuzzification] --- This is simply the reverse process of
  fuzzification. The fuzzy output membership value is converted into a
  discrete output value. There are many defuzzification methods; the
  common ones are \emph{centroid}, \emph{max} and \emph{average of
    max} methods. In the \emph{centroid} method, the output value is
  the centre of gravity of the fuzzy output membership value. In the
  \emph{max} method, the output value is one of the variable values in
  which the fuzzy output membership value is maximum. In the
  \emph{average of max} method, the output value is the average of all
  the variable values in which the fuzzy output membership value is
  maximum.
\end{description}
An example of the inference process is shown in Figure
\ref{fig:InferenceProcess} \cite{FLT07}.

\begin{figure}[htbp]
 \centering
 \includegraphics[width=\textwidth]{InferenceProcess}
 \caption{The inference process of a fuzzy expert system.}
 \label{fig:InferenceProcess}
\end{figure}

\section{Conclusion}
\label{EA.Summary}
This chapter presents a survey on the learning techniques that are
used in this thesis. It begins with a discussion on evolutionary
algorithms (EAs) with an emphasis on the two implementations used in
this thesis: Genetic Programming (GP) and Grammatical Evolution
(GE). It then discusses how EAs can be extended to solve the
multi-objective optimisation problems. The last section discusses
fuzzy expert systems that are built on the ground of fuzzy inference
rules. The fuzzy set concept is used as an example in Section
\ref{FuzzyPolicyEnsembles} to show how other learning techniques can
be used in conjunction with EAs to improve learning performance.
