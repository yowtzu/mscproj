\chapter{Evaluation and Conclusion}
\graphicspath{{Conclusions/figures/}}
\label{EvaluationAndConclusion}
The work reported in previous chapters provides evidence to support
the thesis hypothesis stated in Section
\ref{ThesisHypothesisAndContribution}. The thesis hypothesis is:

\begin{quote}
  Evolutionary algorithms (EAs) have the potential to be an effective
  means of determining the security policies that suit challenging
  environments, as typified by mobile ac-hoc networks (MANETs).
\end{quote}

This chapter reviews the work that has been done, evaluates the extent
that they justify the thesis hypothesis and concludes the thesis by
addressing the directions for future work.

\section{Evaluation}
\label{Evaluation}
The contributions and novelty of the work in this thesis are
summarised as follows.

\subsection{Static Security Policy Inference}
\label{Evaluation.StaticSecurityPolicyInference}
Current security policy is often developed in a top-down
approach. High-level security goals are first determined, after which
they undergo a series of refinement processes to obtain the low-level
executable rules. Although some work has been done in applying machine
learning techniques to aid the policy refinement process, there is no
previous work to my knowledge in the application of EAs or machine
learning techniques in inferring security policy.

Chapter \ref{StaticSecurityPolicyInference} details the experiments in
using EAs to infer security policies from decision examples
made. Here, EAs serve as a tool to generalise a set of low-level
examples to a set of high-level rules. Various simple security
policies have been attempted and inferred successfully. These include
the traditional MLS Bell-LaPadula policy model, budgetised MLS policy
model and Fuzzy MLS policy model. Two different EAs, namely Genetic
Programming (GP) and Grammatical Evolution (GE) were used. In all
cases, it has been found that a minimal amount of design effort and
domain knowledge are required to infer a policy. The only requirement
is to have a good fitness function.
%The main requirement is to have a set of training examples that forms a good representation of the target policy.

The last part of the chapter presents how other machine techniques can
be incorporated into the policy inference framework created. Fuzzy set
concept is used as an example here. Multiple policies are learnt
independently; each of which focuses on inferring a fuzzy rule for a
particular class of decisions (fuzzification). The ultimate output
policy, which is an ensemble of all these policies, is formed using a
weighted voting mechanism (defuzzification). Various experiments have
been carried out using different fuzzification as well as
defuzzification techniques. The results show that there is a
significant improvement on the inference performance.
% process becomes more robust against outlier examples and skewed example sets.

In summary, this chapter shows that the idea of using EAs to infer
security policy from decision examples is feasible.

\subsection{Dynamic Security Policy Inference}
\label{Evaluation.DynamicSecurityPolicyInference}
There will inevitably be times that arise when unseen circumstances
demand a decision during operational time. In some cases the default
automated response may be imperative; in other cases this may be ill
advised. Manual decisions made to override the default one essentially
defines a new policy. Furthermore, even if the optimal security policy
can be developed or inferred automatically, it would eventually become
suboptimal due to the changes in either the operational environment
or security requirements, or in both of them. A requirement is thus
needed in the security policy: it has to be able to continually change
and be updated to suite the operational needs.

Chapter \ref{DynamicSecurityPolicyInference} details the experiments
on dynamic security policy inference. As there is no dynamic security
policy model available and therefore no decision example is available
for us to work with, we designed a dynamic security policy model. This
model is used to generate time varying decision examples for training
and evaluation purposes. In order to be able to cope with changes in
the the target policy, the inference algorithm used must be able to
learn the concept changes from the new examples in an
\emph{incremental} and \emph{timely} manner.

Two novel dynamic learning frameworks based upon multi-objective
evolutionary algorithms (MOEAs) are proposed: One that is based on
Fan's intuition \cite{FW04} and Diversity via Opposite Objectives
(DOO). DOO treats an $N$ objectives optimisation problem as $2N$
objectives optimisation problem by adding an opposing objective for
each of the original objectives. With such a setting, DOO is able to
maintain the diversity among the individuals in the population whilst
optimising the intended objectives. This diversity can aid in
preventing the population from premature convergence and allows the
concept drift in the policy to be continually relearnt.

In summary, Chapter \ref{DynamicSecurityPolicyInference} shows that
EAs are able to infer and continually update
\emph{dynamic} security policy which varies with time from previous
decision examples made.

\subsection{Mission Specific Policy Discovery}
\label{Evaluation.MissionSpecificPolicyDiscovery}
Chapter \ref{MissionSpecificPolicyDiscovery} introduces the notion of
mission specific policy discovery. EAs are used to discover the
security policies that can provide the optimal, or at least excellent,
tradeoffs among security objectives for a specific mission. Here, EAs
serves as an optimisation tool to synthesise the optimal policies, in
terms of achieving the mission as well as security objectives without
violating the constraints given.

Unlike previous experiments, we demonstrate how simulation can be used
to obtain the fitnesses of the policy candidates (feedback on how
well they perform) which are then used in turn to steer the search
direction. Each policy candidate is plugged into a simulated mission
scenario. The scenario is executed and the outcome of the scenario is
measured and used as the fitness of the policy.

The work is a significant deviation from the practice of fitting a
policy a priori without the details of the specific mission being
taken into account. The concept of a ``mission specific policy'' seems
novel. Of course, our approach assumes that same element of
feedback/fitness can be determined via modelling or simulation. This
limits the applicability of the technique. Moreover, simulation is
used for a great many things in military contexts, e.g.,
war-gaming. If simulation is good enough for war-gaming strategies,
why not for security policies?

Instead of aggregating these security objectives using a weighted sum
of fitness functions, MOEAs are used to discover a set of Pareto
optimal policies. MOEAs have the merit of deferring the weight
assignment among the objectives after the set of Pareto optimal
solutions is discovered. This often makes the weight assignment task
easier as the set of Pareto optimal solutions may reveal the
relationship between the objectives.

By using MOEAs, our approach is very flexible, allowing tradeoffs
between a variety of criteria to be explored. The criteria chosen are
exemplary only; one could imagine tradeoffs being explored between a
list of relevant measurements of interest.

To investigate this idea, we first designed a simple scenario in which
there are contradicting security objectives with various constraints.
MOEAs are then used to search for the Pareto optimal policies for each
scenario. The results show that MOEAs are able to discover these
Pareto optimal policies successfully.

\section{Envisaged Future Work}
Numerous possible future works have been identified during the course
of this research. These include:
\begin{enumerate}
\item Policy fusion --- In dynamic coalitions, parties with different
  policies can come together to collaborate. Prior to to the formation
  of dynamic coalitions, each party may have its own security
  policy. An interesting step forward would be to investigate how well
  EAs could be used to combine these security policies together. One
  possible way is to generate decision examples from both existing
  policies and use these examples as the training input for the policy
  inference framework. MOEAs can also be used to discover the Pareto
  optimal set of policy candidates, which are then chosen depending on
  the security requirements. However there are still issues that
  require further investigation. These include:
  \begin{enumerate}
  \item Understanding how to deal with policies which have different
    set of decision-making factors, which may also be measured using
    different scales.
  \item Understanding what are the implicit priorities EAs assigned to
    the conflicting rules, what are the factors that influence the
    priorities and how to control these priorities, etc.
  \end{enumerate}
\item The robustness of a security policy --- The framework proposed
  in this thesis has been shown to be effective in dynamically
  inferring the optimal policy. However, the optimality of a policy is
  not always the only factor in concern; the robustness in performance
  of a security policy in different environments is equally
  important. This is especially important in a pervasive operating
  environment where the deployment of a new policy can be a difficult
  or expensive process. To incorporate this factor into the proposed
  framework, a way to quantify the robustness in performance of a
  security policy is required.
%  One simple way to quantify the
%  robustness in performance of a policy is defined as follows:
%  \begin{enumerate}
%  \item Split the decision examples in the training set into multiple
%    subsets, possibly based on the time when the decision is made.
%  \item Measure the performance of the security policy on the examples
%    in each subset.
%  \item The robustness in performance of the security policy can be
%    estimated with the variance of the performance of the security
%    policy over all the subsets. More robust security policy has
%    smaller variance.
%  \end{enumerate}
  This measure also provides a way to determine the invariant part of
  the optimal policies for different operational environments in
  concern. The determination of this invariant part is doubly useful:
  Firstly, it can serve as a template or testing target in the policy
  development process. Secondly, it can help to protect the security
  policy inference framework from poisoning attack, which attempts to
  mislead the inference process to the favour of the attacker by the
  injection of specially crafted decision examples.
\item Interaction with human inputs --- Whilst rules can be inferred
  from data examples, one can also provide some useful rules of thumbs
  which can define the underlying policies. The research process of
  integrating the human inputs into the continuous learning loop can
  be an interesting research topic.
\item Extensibility --- An immediate step to bring this work further
  is to show that the inference technique can be used to infer
  policies of other types, including obligation policies, management
  policies, etc. In the mission specific policy inference, the
  considered scenario is relatively simple. The scenario only has one
  type of agent in each team and one type of information. A real test
  of this approach would be to embed it within a more realistic
  simulated scenario, with more sophisticated information types, and
  realistic consequence models.
\end{enumerate}

\section{Closing Remarks}
\label{ClosingRemark}
I believe the work reported in this thesis provides sufficient
evidences in supporting the thesis hypothesis. Experiments have shown
that evolutionary algorithms (EAs) are effective means of inferring
security policy and has the potential to make a significant
contribution to the field of policy inference. Everyone accepts that
policy specification is currently difficult, and things are set to
worsen as systems are deployed in ever more complex environments with
increasing sophistication and subtlety of decision-making process
needed. The work reported here shows that this approach has very
considerable promise and I recommend this approach to the research
community for further investigation.

%%% ----------------------------------------------------------------------

% ------------------------------------------------------------------------

%%% Local Variables: 
%%% mode: latex

%%% TeX-master: "../thesis"
%%% End: 
